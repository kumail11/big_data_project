{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqB__a3dAQea",
        "outputId": "06294204-e29a-4a51-b200-eb85b844f1b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytrends\n",
            "  Downloading pytrends-4.9.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.57)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.11/dist-packages (from pytrends) (2.32.3)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.11/dist-packages (from pytrends) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from pytrends) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.18.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->pytrends) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->pytrends) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->pytrends) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->pytrends) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->pytrends) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->pytrends) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.17.0)\n",
            "Downloading pytrends-4.9.2-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pytrends\n",
            "Successfully installed pytrends-4.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pytrends yfinance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "from pytrends.request import TrendReq\n",
        "import pandas as pd\n",
        "import os, time, json\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, round, lit"
      ],
      "metadata": {
        "id": "R9E9FXldH0jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vevu5i6BP2uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_stock_price():\n",
        "    try:\n",
        "        # Fetch stock data for Netflix (NFLX)\n",
        "        stock_df = yf.download(\"NFLX\", period=\"5d\", interval=\"1d\")\n",
        "        # Flatten the column index if multi-level\n",
        "        if stock_df.columns.nlevels > 1:\n",
        "            stock_df.columns = stock_df.columns.get_level_values(0)\n",
        "        print(\"Stock DataFrame:\")\n",
        "        print(stock_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching stock data: {e}\")\n",
        "\n",
        "fetch_stock_price()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJW04I6Ba8G2",
        "outputId": "04f32041-ac99-4300-8767-058916b264c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stock DataFrame:\n",
            "Price             Close         High          Low         Open   Volume\n",
            "Date                                                                   \n",
            "2025-04-28  1110.380005  1114.000000  1082.619995  1100.000000  3831100\n",
            "2025-04-29  1125.640015  1127.810059  1095.479980  1103.920044  3777300\n",
            "2025-04-30  1131.719971  1133.199951  1101.109985  1112.650024  4266700\n",
            "2025-05-01  1133.469971  1142.420044  1111.829956  1122.520020  3499500\n",
            "2025-05-02  1156.489990  1159.439941  1133.319946  1136.660034  3767900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Q-U1bfAa8Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.master(\"local\").appName(\"NetflixStockGenreAnalysis\").getOrCreate()\n",
        "\n",
        "def fetch_stock_price():\n",
        "    try:\n",
        "        # Fetch stock data for Netflix (NFLX)\n",
        "        stock_df = yf.download(\"NFLX\", period=\"5d\", interval=\"1d\")\n",
        "\n",
        "        if stock_df.columns.nlevels > 1:\n",
        "            stock_df.columns = stock_df.columns.get_level_values(0)\n",
        "        # print(\"Stock DataFrame:\")\n",
        "        # print(stock_df)\n",
        "\n",
        "        # Access the 'Close' column correctly (multi-level index)\n",
        "        close_prices = stock_df['Close']\n",
        "\n",
        "        # Calculate the average close price\n",
        "        avg_close_price = close_prices.mean()\n",
        "        return stock_df, avg_close_price\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching stock data: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "\n",
        "# ✅ Fetch Google Trends Interest\n",
        "def fetch_trend_interest():\n",
        "    try:\n",
        "        pytrends = TrendReq()\n",
        "        pytrends.build_payload(kw_list=[\"Netflix\"], timeframe=\"now 7-d\")\n",
        "        trend_df = pytrends.interest_over_time()\n",
        "        if trend_df.empty:\n",
        "            print(\"⚠️ Trend data is empty!\")\n",
        "            return None, 0\n",
        "        print(\"\\ntrend data:\\n\", trend_df)\n",
        "        trend_df = trend_df.reset_index()\n",
        "        interest_scores = trend_df['Netflix']\n",
        "        avg_trend_interest = interest_scores.mean()\n",
        "        return trend_df, avg_trend_interest\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error fetching trend data: {e}\")\n",
        "        return None, 0\n",
        "\n",
        "\n",
        "\n",
        "base_df_data = [\n",
        "    (\"Comedy\", \"Youth\"),\n",
        "    (\"Thriller\", \"Youth\"),\n",
        "    (\"Drama\", \"Adult\"),\n",
        "    (\"Action\", \"Adult\")\n",
        "]\n",
        "base_df = spark.createDataFrame(base_df_data, [\"genre\", \"audience\"])\n",
        "\n",
        "# Fetch stock and trend data\n",
        "stock_df, avg_price = fetch_stock_price()\n",
        "trend_df, avg_interest = fetch_trend_interest()\n",
        "\n",
        "# Check if data is valid before proceeding\n",
        "if avg_price is None or avg_price == 0.0 or avg_interest is None or avg_interest == 0:\n",
        "    print(\"⚠️ Invalid data, please check the data fetching functions.\")\n",
        "else:\n",
        "    # Add static values as columns\n",
        "    df = base_df.withColumn(\"avg_price\", lit(avg_price))  # Use lit() for scalar values\n",
        "    df = df.withColumn(\"avg_interest\", lit(avg_interest))  # Use lit() for scalar values\n",
        "\n",
        "    # Add interest_level\n",
        "    df = df.withColumn(\n",
        "        \"interest_level\",\n",
        "        when(col(\"avg_interest\") >= 81, \"High\")\n",
        "        .when(col(\"avg_interest\") >= 70, \"Moderate\")\n",
        "        .otherwise(\"Low\")\n",
        "    )\n",
        "\n",
        "    # Add price_level\n",
        "    df = df.withColumn(\n",
        "        \"price_level\",\n",
        "        when(col(\"avg_price\") >= 440, \"High\")\n",
        "        .when(col(\"avg_price\") >= 435, \"Moderate\")\n",
        "        .otherwise(\"Low\")\n",
        "    )\n",
        "\n",
        "    # Show Result in Colab\n",
        "    print(\"\\nFinal PySpark DataFrame:\")\n",
        "    df.show(truncate=False)\n",
        "\n",
        "    # Save to CSV\n",
        "    df_pd = df.toPandas()\n",
        "    df_pd.to_csv(\"netflix_genre_analysis.csv\", index=False)\n",
        "    print(\"✅ CSV saved as 'netflix_genre_analysis.csv'\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n",
        "\n",
        "\n",
        "# stock_df, avg_close_price = fetch_stock_price()\n",
        "# trend_df, avg_trend_interest = fetch_trend_interest()\n",
        "\n",
        "# if stock_df is not None and trend_df is not None:\n",
        "#     # Add avg_trend_interest as a column to stock_df\n",
        "#     stock_df['avg_trend_interest'] = avg_trend_interest\n",
        "\n",
        "#     # Print results\n",
        "#     print(\"Stock DataFrame with avg_trend_interest:\")\n",
        "#     print(stock_df)\n",
        "#     print(f\"\\nCalculated Average Close Price: {avg_close_price}\")\n",
        "#     print(f\"Calculated Average Trend Interest: {avg_trend_interest}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjUCdw-9Ul5A",
        "outputId": "35322237-9aaf-4113-c8ab-76079e04a2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "trend data:\n",
            "                      Netflix  isPartial\n",
            "date                                   \n",
            "2025-04-27 18:00:00       82      False\n",
            "2025-04-27 19:00:00       86      False\n",
            "2025-04-27 20:00:00       80      False\n",
            "2025-04-27 21:00:00       81      False\n",
            "2025-04-27 22:00:00       76      False\n",
            "...                      ...        ...\n",
            "2025-05-04 14:00:00       59      False\n",
            "2025-05-04 15:00:00       63      False\n",
            "2025-05-04 16:00:00       68      False\n",
            "2025-05-04 17:00:00       73      False\n",
            "2025-05-04 18:00:00       88       True\n",
            "\n",
            "[169 rows x 2 columns]\n",
            "\n",
            "Final PySpark DataFrame:\n",
            "+--------+--------+-----------------+-----------------+--------------+-----------+\n",
            "|genre   |audience|avg_price        |avg_interest     |interest_level|price_level|\n",
            "+--------+--------+-----------------+-----------------+--------------+-----------+\n",
            "|Comedy  |Youth   |1131.539990234375|57.79289940828402|Low           |High       |\n",
            "|Thriller|Youth   |1131.539990234375|57.79289940828402|Low           |High       |\n",
            "|Drama   |Adult   |1131.539990234375|57.79289940828402|Low           |High       |\n",
            "|Action  |Adult   |1131.539990234375|57.79289940828402|Low           |High       |\n",
            "+--------+--------+-----------------+-----------------+--------------+-----------+\n",
            "\n",
            "✅ CSV saved as 'netflix_genre_analysis.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ae-UcJS_Ul9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pL9wngbrUmAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQ-7v2JcUmCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3X7_p9o2UmEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_MJQGc2kUmHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"/content/stream_input\"\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "qwtKrrdbIA-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genre_data = [\n",
        "    {\"genre\": \"Comedy\", \"audience\": \"Youth\"},\n",
        "    {\"genre\": \"Thriller\", \"audience\": \"Youth\"},\n",
        "    {\"genre\": \"Documentary\", \"audience\": \"Adults\"},\n",
        "    {\"genre\": \"Romance\", \"audience\": \"Youth\"},\n",
        "    {\"genre\": \"Action\", \"audience\": \"All\"},\n",
        "    {\"genre\": \"Horror\", \"audience\": \"Adults\"}\n",
        "]\n",
        "\n",
        "base_df = spark.createDataFrame(genre_data)\n",
        "\n",
        "# ✅ Fetch Netflix Stock Price\n",
        "def fetch_stock_price():\n",
        "    try:\n",
        "        stock_df = yf.download(\"NFLX\", period=\"5d\", interval=\"1d\")\n",
        "        if stock_df.empty:\n",
        "            print(\"⚠️ Stock data is empty!\")\n",
        "            return 0.0\n",
        "        print(\"\\nstock data:\\n\", stock_df)\n",
        "        return round(stock_df[\"Close\"].mean(), 2)  # calculate the average Close price\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error fetching stock data: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "# ✅ Fetch Google Trends Interest\n",
        "def fetch_trend_interest():\n",
        "    try:\n",
        "        pytrends = TrendReq()\n",
        "        pytrends.build_payload(kw_list=[\"Netflix\"], timeframe=\"now 7-d\")\n",
        "        trend_df = pytrends.interest_over_time()\n",
        "        if trend_df.empty:\n",
        "            print(\"⚠️ Trend data is empty!\")\n",
        "            return 0\n",
        "        print(\"\\ntrend data:\\n\", trend_df)\n",
        "        return round(trend_df[\"Netflix\"].mean(), 0)  # calculate the average interest\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error fetching trend data: {e}\")\n",
        "        return 0\n",
        "\n",
        "# ✅ Fetch data dynamically\n",
        "avg_price = fetch_stock_price()\n",
        "avg_interest = fetch_trend_interest()\n",
        "\n",
        "# ✅ Check if data is valid before proceeding\n",
        "if avg_price == 0.0 or avg_interest == 0:\n",
        "    print(\"⚠️ Invalid data, please check the data fetching functions.\")\n",
        "\n",
        "# ✅ Add static values as columns\n",
        "df = base_df.withColumn(\"avg_price\", lit(avg_price))  # Use lit() for scalar values\n",
        "df = df.withColumn(\"avg_interest\", lit(avg_interest))  # Use lit() for scalar values\n",
        "\n",
        "# ✅ Add interest_level\n",
        "df = df.withColumn(\n",
        "    \"interest_level\",\n",
        "    when(col(\"avg_interest\") >= 81, \"High\")\n",
        "    .when(col(\"avg_interest\") >= 70, \"Moderate\")\n",
        "    .otherwise(\"Low\")\n",
        ")\n",
        "\n",
        "# ✅ Add price_level\n",
        "df = df.withColumn(\n",
        "    \"price_level\",\n",
        "    when(col(\"avg_price\") >= 440, \"High\")\n",
        "    .when(col(\"avg_price\") >= 435, \"Moderate\")\n",
        "    .otherwise(\"Low\")\n",
        ")\n",
        "\n",
        "# ✅ Show Result in Colab\n",
        "df.show(truncate=False)\n",
        "\n",
        "# ✅ Save to CSV\n",
        "df_pd = df.toPandas()\n",
        "df_pd.to_csv(\"netflix_genre_analysis.csv\", index=False)\n",
        "print(\"✅ CSV saved as 'netflix_genre_analysis.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icUrmheRII_o",
        "outputId": "c9bc3185-409d-408f-c05b-c4fe61f1c887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "stock data:\n",
            " Price             Close         High          Low         Open   Volume\n",
            "Ticker             NFLX         NFLX         NFLX         NFLX     NFLX\n",
            "Date                                                                   \n",
            "2025-04-28  1110.380005  1114.000000  1082.619995  1100.000000  3831100\n",
            "2025-04-29  1125.640015  1127.810059  1095.479980  1103.920044  3777300\n",
            "2025-04-30  1131.719971  1133.199951  1101.109985  1112.650024  4266700\n",
            "2025-05-01  1133.469971  1142.420044  1111.829956  1122.520020  3499500\n",
            "2025-05-02  1156.489990  1159.439941  1133.319946  1136.660034  3767900\n",
            "⚠️ Error fetching stock data: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got Series.\n",
            "\n",
            "trend data:\n",
            "                      Netflix  isPartial\n",
            "date                                   \n",
            "2025-04-27 17:00:00       66      False\n",
            "2025-04-27 18:00:00       77      False\n",
            "2025-04-27 19:00:00       84      False\n",
            "2025-04-27 20:00:00       83      False\n",
            "2025-04-27 21:00:00       78      False\n",
            "...                      ...        ...\n",
            "2025-05-04 13:00:00       54      False\n",
            "2025-05-04 14:00:00       59      False\n",
            "2025-05-04 15:00:00       59      False\n",
            "2025-05-04 16:00:00       64      False\n",
            "2025-05-04 17:00:00       72       True\n",
            "\n",
            "[169 rows x 2 columns]\n",
            "⚠️ Error fetching trend data: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got float64.\n",
            "⚠️ Invalid data, please check the data fetching functions.\n",
            "+--------+-----------+---------+------------+--------------+-----------+\n",
            "|audience|genre      |avg_price|avg_interest|interest_level|price_level|\n",
            "+--------+-----------+---------+------------+--------------+-----------+\n",
            "|Youth   |Comedy     |0.0      |0           |Low           |Low        |\n",
            "|Youth   |Thriller   |0.0      |0           |Low           |Low        |\n",
            "|Adults  |Documentary|0.0      |0           |Low           |Low        |\n",
            "|Youth   |Romance    |0.0      |0           |Low           |Low        |\n",
            "|All     |Action     |0.0      |0           |Low           |Low        |\n",
            "|Adults  |Horror     |0.0      |0           |Low           |Low        |\n",
            "+--------+-----------+---------+------------+--------------+-----------+\n",
            "\n",
            "✅ CSV saved as 'netflix_genre_analysis.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_eLyVagPKdWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"NetflixStreamingAnalysis\").getOrCreate()\n",
        "\n",
        "# Read streaming data from folder\n",
        "df = spark.readStream.schema(\"\"\"\n",
        "    timestamp STRING,\n",
        "    genre STRING,\n",
        "    audience STRING,\n",
        "    avg_price DOUBLE,\n",
        "    avg_interest DOUBLE\n",
        "\"\"\").json(\"/content/stream_input\")\n",
        "\n",
        "# Add column\n",
        "df = df.withColumn(\n",
        "    \"interest_level\",\n",
        "    when(col(\"avg_interest\") >= 81, \"High\")\n",
        "    .when(col(\"avg_interest\") >= 70, \"Moderate\")\n",
        "    .otherwise(\"Low\")\n",
        ")\n",
        "\n",
        "df = df.withColumn(\n",
        "    \"price_level\",\n",
        "    when(col(\"avg_price\") >= 440, \"High\")\n",
        "    .when(col(\"avg_price\") >= 435, \"Moderate\")\n",
        "    .otherwise(\"Low\")\n",
        ")\n",
        "\n",
        "# Output to console log\n",
        "query = df.writeStream.outputMode(\"append\").format(\"console\").option(\"truncate\", False).start()"
      ],
      "metadata": {
        "id": "4y7gcpVAMC1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EVNvk0WDfAFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kn60BUBQfAHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5MYNbVGpfAJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "# !pip install yfinance pytrends pyspark\n",
        "\n",
        "import yfinance as yf\n",
        "from pytrends.request import TrendReq\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, when, sum as sum_\n",
        "import time\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"NetflixGenreAnalysis\").getOrCreate()\n",
        "\n",
        "# Function to fetch stock price\n",
        "def fetch_stock_price():\n",
        "    try:\n",
        "        stock_df = yf.download(\"NFLX\", period=\"5d\", interval=\"1d\")\n",
        "        print(\"stock data:\\n\", stock_df)\n",
        "        if stock_df.columns.nlevels > 1:\n",
        "            stock_df.columns = stock_df.columns.get_level_values(0)\n",
        "        close_prices = stock_df['Close']\n",
        "        avg_close_price = close_prices.mean()\n",
        "        return stock_df, avg_close_price\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching stock data: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Function to fetch trend interest for multiple genres\n",
        "def fetch_trend_interest(genres):\n",
        "    try:\n",
        "        pytrends = TrendReq(hl='en-US', tz=360)\n",
        "        trend_data = []\n",
        "        for genre in genres:\n",
        "            keyword = f\"Netflix {genre}\"\n",
        "            pytrends.build_payload(kw_list=[keyword], timeframe=\"now 7-d\")\n",
        "            trend_df = pytrends.interest_over_time()\n",
        "            time.sleep(2)  # Avoid rate limits\n",
        "            if trend_df.empty:\n",
        "                print(f\"⚠️ Trend data for {keyword} is empty!\")\n",
        "                avg_interest = 10.0  # Fallback value\n",
        "            else:\n",
        "                avg_interest = trend_df[keyword].mean()\n",
        "            trend_data.append({\"genre\": genre, \"avg_interest\": avg_interest})\n",
        "        return pd.DataFrame(trend_data)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error fetching trend data: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Define genres and audience mapping\n",
        "genres = [\"Comedy\", \"Thriller\", \"Drama\", \"Action\"]\n",
        "genre_audience_map = {\n",
        "    \"Comedy\": \"Youth\",\n",
        "    \"Thriller\": \"Youth\",\n",
        "    \"Drama\": \"Adult\",\n",
        "    \"Action\": \"Adult\"\n",
        "}\n",
        "\n",
        "# Fetch stock price\n",
        "stock_df, avg_price = fetch_stock_price()\n",
        "print(f\"\\nAverage Stock Price: {avg_price}\")\n",
        "\n",
        "# Fetch trend interest for each genre\n",
        "trend_df_pandas = fetch_trend_interest(genres)\n",
        "\n",
        "# Create base_df with genre and audience\n",
        "base_df_data = [(genre, genre_audience_map[genre]) for genre in genres]\n",
        "base_df = spark.createDataFrame(base_df_data, [\"genre\", \"audience\"])\n",
        "\n",
        "# Convert trend_df_pandas to PySpark DataFrame\n",
        "if not trend_df_pandas.empty:\n",
        "    trend_df = spark.createDataFrame(trend_df_pandas)\n",
        "else:\n",
        "    print(\"⚠️ No trend data available!\")\n",
        "    trend_df = spark.createDataFrame([(genre, 10.0) for genre in genres], [\"genre\", \"avg_interest\"])\n",
        "\n",
        "# Join base_df with trend_df\n",
        "df = base_df.join(trend_df, \"genre\", \"left\")\n",
        "\n",
        "# Check if data is valid\n",
        "if avg_price is None or avg_price == 0.0 or df.filter(col(\"avg_interest\") > 0).count() == 0:\n",
        "    print(\"⚠️ Invalid data, please check the data fetching functions.\")\n",
        "else:\n",
        "    # Calculate total interest for normalization\n",
        "    total_interest = df.select(sum_(col(\"avg_interest\")).alias(\"total_interest\")).collect()[0][\"total_interest\"]\n",
        "\n",
        "    # Add weight based on avg_interest\n",
        "    df = df.withColumn(\"weight\", col(\"avg_interest\") / total_interest)\n",
        "\n",
        "    # Adjust avg_price based on weight\n",
        "    df = df.withColumn(\"avg_price\", lit(avg_price) * col(\"weight\"))\n",
        "\n",
        "    # Add interest_level\n",
        "    df = df.withColumn(\n",
        "        \"interest_level\",\n",
        "        when(col(\"avg_interest\") >= 60, \"High\")\n",
        "        .when(col(\"avg_interest\") >= 50, \"Moderate\")\n",
        "        .otherwise(\"Low\")\n",
        "    )\n",
        "\n",
        "    # Add price_level\n",
        "    df = df.withColumn(\n",
        "        \"price_level\",\n",
        "        when(col(\"avg_price\") >= 400, \"High\")\n",
        "        .when(col(\"avg_price\") >= 300, \"Moderate\")\n",
        "        .otherwise(\"Low\")\n",
        "    )\n",
        "\n",
        "    # Select final columns\n",
        "    df = df.select(\"genre\", \"audience\", \"avg_price\", \"avg_interest\", \"interest_level\", \"price_level\")\n",
        "\n",
        "    # Show Result\n",
        "    print(\"Final PySpark DataFrame:\")\n",
        "    df.show(truncate=False)\n",
        "\n",
        "    # Save to CSV\n",
        "    df_pd = df.toPandas()\n",
        "    df_pd.to_csv(\"netflix_genre_analysis.csv\", index=False)\n",
        "    print(\"✅ CSV saved as 'netflix_genre_analysis.csv'\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjAUDT4rfANR",
        "outputId": "4913b0e4-d8ba-42e2-a31e-fb833f43607c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['NFLX']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stock data:\n",
            " Empty DataFrame\n",
            "Columns: [(Adj Close, NFLX), (Close, NFLX), (High, NFLX), (Low, NFLX), (Open, NFLX), (Volume, NFLX)]\n",
            "Index: []\n",
            "\n",
            "Average Stock Price: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final PySpark DataFrame:\n",
            "+--------+--------+---------+-----------------+--------------+-----------+\n",
            "|genre   |audience|avg_price|avg_interest     |interest_level|price_level|\n",
            "+--------+--------+---------+-----------------+--------------+-----------+\n",
            "|Thriller|Youth   |NaN      |36.55029585798817|Low           |High       |\n",
            "|Comedy  |Youth   |NaN      |32.0414201183432 |Low           |High       |\n",
            "|Drama   |Adult   |NaN      |64.16568047337279|High          |High       |\n",
            "|Action  |Adult   |NaN      |53.82248520710059|Moderate      |High       |\n",
            "+--------+--------+---------+-----------------+--------------+-----------+\n",
            "\n",
            "✅ CSV saved as 'netflix_genre_analysis.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZzB5njsBfBao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xVx8MATzm5m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2nd approach"
      ],
      "metadata": {
        "id": "AXcJuMUvm619"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "# !pip install yfinance pytrends pyspark\n",
        "\n",
        "import yfinance as yf\n",
        "from pytrends.request import TrendReq\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, when, sum as sum_\n",
        "import time\n",
        "import random\n",
        "import requests\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"NetflixGenreAnalysis\").getOrCreate()\n",
        "\n",
        "# Function to fetch stock price with retry and User-Agent rotation\n",
        "def fetch_stock_price(ticker=\"NFLX\", retries=3, initial_delay=5):\n",
        "    user_agents = [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15',\n",
        "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'\n",
        "    ]\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            # Set random User-Agent\n",
        "            yf.pdr_override()  # Ensure compatibility\n",
        "            session = requests.Session()\n",
        "            session.headers.update({'User-Agent': random.choice(user_agents)})\n",
        "            stock_df = yf.download(ticker, period=\"5d\", interval=\"1d\", session=session)\n",
        "            if stock_df.empty:\n",
        "                print(f\"⚠️ Empty stock data for {ticker}!\")\n",
        "                raise ValueError(\"Empty DataFrame\")\n",
        "            if stock_df.columns.nlevels > 1:\n",
        "                stock_df.columns = stock_df.columns.get_level_values(0)\n",
        "            close_prices = stock_df['Close']\n",
        "            avg_close_price = close_prices.mean()\n",
        "            return stock_df, avg_close_price\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1}/{retries} failed for {ticker}: {e}\")\n",
        "            if attempt < retries - 1:\n",
        "                delay = initial_delay * (2 ** attempt)  # Exponential backoff\n",
        "                print(f\"Retrying after {delay} seconds...\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                print(f\"⚠️ Failed to fetch stock data for {ticker} after {retries} attempts.\")\n",
        "                return None, None\n",
        "\n",
        "# Function to fetch trend interest for multiple genres\n",
        "def fetch_trend_interest(genres):\n",
        "    try:\n",
        "        pytrends = TrendReq(hl='en-US', tz=360)\n",
        "        trend_data = []\n",
        "        for genre in genres:\n",
        "            keyword = f\"Netflix {genre}\"\n",
        "            pytrends.build_payload(kw_list=[keyword], timeframe=\"now 7-d\")\n",
        "            time.sleep(2)  # Avoid PyTrends rate limits\n",
        "            trend_df = pytrends.interest_over_time()\n",
        "            if trend_df.empty:\n",
        "                print(f\"⚠️ Trend data for {keyword} is empty!\")\n",
        "                avg_interest = 10.0  # Fallback value\n",
        "            else:\n",
        "                avg_interest = trend_df[keyword].mean()\n",
        "            trend_data.append({\"genre\": genre, \"avg_interest\": avg_interest})\n",
        "        return pd.DataFrame(trend_data)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error fetching trend data: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Define genres and audience mapping\n",
        "genres = [\"Comedy\", \"Thriller\", \"Drama\", \"Action\"]\n",
        "genre_audience_map = {\n",
        "    \"Comedy\": \"Youth\",\n",
        "    \"Thriller\": \"Youth\",\n",
        "    \"Drama\": \"Adult\",\n",
        "    \"Action\": \"Adult\"\n",
        "}\n",
        "\n",
        "# Fetch stock price\n",
        "stock_df, avg_price = fetch_stock_price()\n",
        "if avg_price is None:\n",
        "    print(\"⚠️ Using fallback avg_price due to fetch failure.\")\n",
        "    avg_price = 436.22  # Fallback to align with your example\n",
        "\n",
        "# Fetch trend interest for each genre\n",
        "trend_df_pandas = fetch_trend_interest(genres)\n",
        "\n",
        "# Create base_df with genre and audience\n",
        "base_df_data = [(genre, genre_audience_map[genre]) for genre in genres]\n",
        "base_df = spark.createDataFrame(base_df_data, [\"genre\", \"audience\"])\n",
        "\n",
        "# Convert trend_df_pandas to PySpark DataFrame\n",
        "if not trend_df_pandas.empty:\n",
        "    trend_df = spark.createDataFrame(trend_df_pandas)\n",
        "else:\n",
        "    print(\"⚠️ No trend data available!\")\n",
        "    trend_df = spark.createDataFrame([(genre, 10.0) for genre in genres], [\"genre\", \"avg_interest\"])\n",
        "\n",
        "# Join base_df with trend_df\n",
        "df = base_df.join(trend_df, \"genre\", \"left\")\n",
        "\n",
        "# Check if data is valid\n",
        "if avg_price == 0.0 or df.filter(col(\"avg_interest\") > 0).count() == 0:\n",
        "    print(\"⚠️ Invalid data, please check the data fetching functions.\")\n",
        "else:\n",
        "    # Calculate total interest for normalization\n",
        "    total_interest = df.select(sum_(col(\"avg_interest\")).alias(\"total_interest\")).collect()[0][\"total_interest\"]\n",
        "\n",
        "    # Add weight based on avg_interest\n",
        "    df = df.withColumn(\"weight\", col(\"avg_interest\") / total_interest)\n",
        "\n",
        "    # Adjust avg_price based on weight\n",
        "    df = df.withColumn(\"avg_price\", lit(avg_price) * col(\"weight\"))\n",
        "\n",
        "    # Add interest_level\n",
        "    df = df.withColumn(\n",
        "        \"interest_level\",\n",
        "        when(col(\"avg_interest\") >= 60, \"High\")\n",
        "        .when(col(\"avg_interest\") >= 50, \"Moderate\")\n",
        "        .otherwise(\"Low\")\n",
        "    )\n",
        "\n",
        "    # Add price_level\n",
        "    df = df.withColumn(\n",
        "        \"price_level\",\n",
        "        when(col(\"avg_price\") >= 400, \"High\")\n",
        "        .when(col(\"avg_price\") >= 300, \"Moderate\")\n",
        "        .otherwise(\"Low\")\n",
        "    )\n",
        "\n",
        "    # Select final columns\n",
        "    df = df.select(\"genre\", \"audience\", \"avg_price\", \"avg_interest\", \"interest_level\", \"price_level\")\n",
        "\n",
        "    # Show Result\n",
        "    print(\"Final PySpark DataFrame:\")\n",
        "    df.show(truncate=False)\n",
        "\n",
        "    # Save to CSV\n",
        "    df_pd = df.toPandas()\n",
        "    df_pd.to_csv(\"netflix_genre_analysis.csv\", index=False)\n",
        "    print(\"✅ CSV saved as 'netflix_genre_analysis.csv'\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLawSehdm51o",
        "outputId": "ec6b843f-cd0c-4b75-8f0b-db482f229e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempt 1/3 failed for NFLX: module 'yfinance' has no attribute 'pdr_override'\n",
            "Retrying after 5 seconds...\n",
            "Attempt 2/3 failed for NFLX: module 'yfinance' has no attribute 'pdr_override'\n",
            "Retrying after 10 seconds...\n",
            "Attempt 3/3 failed for NFLX: module 'yfinance' has no attribute 'pdr_override'\n",
            "⚠️ Failed to fetch stock data for NFLX after 3 attempts.\n",
            "⚠️ Using fallback avg_price due to fetch failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final PySpark DataFrame:\n",
            "+--------+--------+------------------+------------------+--------------+-----------+\n",
            "|genre   |audience|avg_price         |avg_interest      |interest_level|price_level|\n",
            "+--------+--------+------------------+------------------+--------------+-----------+\n",
            "|Thriller|Youth   |85.3903935309973  |36.52662721893491 |Low           |Low        |\n",
            "|Comedy  |Youth   |74.97423180592993 |32.071005917159766|Low           |Low        |\n",
            "|Drama   |Adult   |150.00379514824797|64.16568047337279 |High          |Low        |\n",
            "|Action  |Adult   |125.8515795148248 |53.83431952662722 |Moderate      |Low        |\n",
            "+--------+--------+------------------+------------------+--------------+-----------+\n",
            "\n",
            "✅ CSV saved as 'netflix_genre_analysis.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eOLwc_T9m-sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CqHLUamBoGJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "# !pip install yfinance pytrends pyspark\n",
        "\n",
        "import yfinance as yf\n",
        "from pytrends.request import TrendReq\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, when, sum as sum_\n",
        "import time\n",
        "import random\n",
        "import requests\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"NetflixGenreAnalysis\").getOrCreate()\n",
        "\n",
        "# Function to fetch stock price with retry and User-Agent rotation\n",
        "def fetch_stock_price(ticker=\"NFLX\", retries=3, initial_delay=5):\n",
        "    user_agents = [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15',\n",
        "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'\n",
        "    ]\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            # Create a session with random User-Agent\n",
        "            session = requests.Session()\n",
        "            session.headers.update({'User-Agent': random.choice(user_agents)})\n",
        "            # Fetch stock data using yfinance with custom session\n",
        "            stock_df = yf.download(ticker, period=\"5d\", interval=\"1d\", session=session)\n",
        "            print(\"\\nstock dataframe:\\n\", stock_df)\n",
        "            if stock_df.empty:\n",
        "                print(f\"⚠️ Empty stock data for {ticker}!\")\n",
        "                raise ValueError(\"Empty DataFrame\")\n",
        "            if stock_df.columns.nlevels > 1:\n",
        "                stock_df.columns = stock_df.columns.get_level_values(0)\n",
        "            close_prices = stock_df['Close']\n",
        "            avg_close_price = close_prices.mean()\n",
        "            return stock_df, avg_close_price\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1}/{retries} failed for {ticker}: {e}\")\n",
        "            if attempt < retries - 1:\n",
        "                delay = initial_delay * (2 ** attempt)  # Exponential backoff\n",
        "                print(f\"Retrying after {delay} seconds...\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                print(f\"⚠️ Failed to fetch stock data for {ticker} after {retries} attempts.\")\n",
        "                return None, None\n",
        "\n",
        "# Function to fetch trend interest for multiple genres\n",
        "def fetch_trend_interest(genres):\n",
        "    try:\n",
        "        pytrends = TrendReq(hl='en-US', tz=360)\n",
        "        trend_data = []\n",
        "        for genre in genres:\n",
        "            keyword = f\"Netflix {genre}\"\n",
        "            pytrends.build_payload(kw_list=[keyword], timeframe=\"now 7-d\")\n",
        "            time.sleep(2)  # Avoid PyTrends rate limits\n",
        "            trend_df = pytrends.interest_over_time()\n",
        "            if trend_df.empty:\n",
        "                print(f\"⚠️ Trend data for {keyword} is empty!\")\n",
        "                avg_interest = 10.0  # Fallback value\n",
        "            else:\n",
        "                avg_interest = trend_df[keyword].mean()\n",
        "            trend_data.append({\"genre\": genre, \"avg_interest\": avg_interest})\n",
        "        return pd.DataFrame(trend_data)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error fetching trend data: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Define genres and audience mapping\n",
        "genres = [\"Comedy\", \"Thriller\", \"Drama\", \"Action\"]\n",
        "genre_audience_map = {\n",
        "    \"Comedy\": \"Youth\",\n",
        "    \"Thriller\": \"Youth\",\n",
        "    \"Drama\": \"Adult\",\n",
        "    \"Action\": \"Adult\"\n",
        "}\n",
        "\n",
        "# Fetch stock price\n",
        "stock_df, avg_price = fetch_stock_price()\n",
        "print(f\"\\nbefore avg price: {avg_price}\")\n",
        "if avg_price is None:\n",
        "    print(\"⚠️ Using fallback avg_price due to fetch failure.\")\n",
        "    avg_price = 436.22  # Fallback to align with your example\n",
        "\n",
        "print(f\"\\nafter avg price: {avg_price}\")\n",
        "\n",
        "# Fetch trend interest for each genre\n",
        "trend_df_pandas = fetch_trend_interest(genres)\n",
        "\n",
        "# Create base_df with genre and audience\n",
        "base_df_data = [(genre, genre_audience_map[genre]) for genre in genres]\n",
        "base_df = spark.createDataFrame(base_df_data, [\"genre\", \"audience\"])\n",
        "\n",
        "# Convert trend_df_pandas to PySpark DataFrame\n",
        "if not trend_df_pandas.empty:\n",
        "    trend_df = spark.createDataFrame(trend_df_pandas)\n",
        "else:\n",
        "    print(\"⚠️ No trend data available!\")\n",
        "    trend_df = spark.createDataFrame([(genre, 10.0) for genre in genres], [\"genre\", \"avg_interest\"])\n",
        "\n",
        "# Join base_df with trend_df\n",
        "df = base_df.join(trend_df, \"genre\", \"left\")\n",
        "\n",
        "# Check if data is valid\n",
        "if avg_price == 0.0 or df.filter(col(\"avg_interest\") > 0).count() == 0:\n",
        "    print(\"⚠️ Invalid data, please check the data fetching functions.\")\n",
        "else:\n",
        "    # Calculate total interest for normalization\n",
        "    total_interest = df.select(sum_(col(\"avg_interest\")).alias(\"total_interest\")).collect()[0][\"total_interest\"]\n",
        "\n",
        "    # Add weight based on avg_interest\n",
        "    df = df.withColumn(\"weight\", col(\"avg_interest\") / total_interest)\n",
        "\n",
        "    # Adjust avg_price based on weight\n",
        "    df = df.withColumn(\"avg_price\", lit(avg_price) * col(\"weight\"))\n",
        "\n",
        "    # Add interest_level\n",
        "    df = df.withColumn(\n",
        "        \"interest_level\",\n",
        "        when(col(\"avg_interest\") >= 60, \"High\")\n",
        "        .when(col(\"avg_interest\") >= 50, \"Moderate\")\n",
        "        .otherwise(\"Low\")\n",
        "    )\n",
        "\n",
        "    # Add price_level\n",
        "    df = df.withColumn(\n",
        "        \"price_level\",\n",
        "        when(col(\"avg_price\") >= 400, \"High\")\n",
        "        .when(col(\"avg_price\") >= 300, \"Moderate\")\n",
        "        .otherwise(\"Low\")\n",
        "    )\n",
        "\n",
        "    # Select final columns\n",
        "    df = df.select(\"genre\", \"audience\", \"avg_price\", \"avg_interest\", \"interest_level\", \"price_level\")\n",
        "\n",
        "    # Show Result\n",
        "    print(\"Final PySpark DataFrame:\")\n",
        "    df.show(truncate=False)\n",
        "\n",
        "    # Save to CSV\n",
        "    df_pd = df.toPandas()\n",
        "    df_pd.to_csv(\"netflix_genre_analysis.csv\", index=False)\n",
        "    print(\"✅ CSV saved as 'netflix_genre_analysis.csv'\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYesGmHdoGUg",
        "outputId": "1d4b77c0-1786-40de-8b0c-5b69420ba64a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['NFLX']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "stock dataframe:\n",
            " Empty DataFrame\n",
            "Columns: [(Adj Close, NFLX), (Close, NFLX), (High, NFLX), (Low, NFLX), (Open, NFLX), (Volume, NFLX)]\n",
            "Index: []\n",
            "⚠️ Empty stock data for NFLX!\n",
            "Attempt 1/3 failed for NFLX: Empty DataFrame\n",
            "Retrying after 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['NFLX']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "stock dataframe:\n",
            " Empty DataFrame\n",
            "Columns: [(Adj Close, NFLX), (Close, NFLX), (High, NFLX), (Low, NFLX), (Open, NFLX), (Volume, NFLX)]\n",
            "Index: []\n",
            "⚠️ Empty stock data for NFLX!\n",
            "Attempt 2/3 failed for NFLX: Empty DataFrame\n",
            "Retrying after 10 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['NFLX']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "stock dataframe:\n",
            " Empty DataFrame\n",
            "Columns: [(Adj Close, NFLX), (Close, NFLX), (High, NFLX), (Low, NFLX), (Open, NFLX), (Volume, NFLX)]\n",
            "Index: []\n",
            "⚠️ Empty stock data for NFLX!\n",
            "Attempt 3/3 failed for NFLX: Empty DataFrame\n",
            "⚠️ Failed to fetch stock data for NFLX after 3 attempts.\n",
            "\n",
            "before avg price: None\n",
            "⚠️ Using fallback avg_price due to fetch failure.\n",
            "\n",
            "after avg price: 436.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final PySpark DataFrame:\n",
            "+--------+--------+------------------+------------------+--------------+-----------+\n",
            "|genre   |audience|avg_price         |avg_interest      |interest_level|price_level|\n",
            "+--------+--------+------------------+------------------+--------------+-----------+\n",
            "|Thriller|Youth   |87.45045505420357 |37.094674556213015|Low           |Low        |\n",
            "|Comedy  |Youth   |77.51829298711266 |32.88165680473373 |Low           |Low        |\n",
            "|Drama   |Adult   |145.89955485913467|61.887573964497044|High          |Low        |\n",
            "|Action  |Adult   |125.35169709954911|53.171597633136095|Moderate      |Low        |\n",
            "+--------+--------+------------------+------------------+--------------+-----------+\n",
            "\n",
            "✅ CSV saved as 'netflix_genre_analysis.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sCts8LVqoIw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KGMucNuF7Op6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "# !pip install yfinance pytrends pyspark\n",
        "\n",
        "import yfinance as yf\n",
        "from pytrends.request import TrendReq\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, when, sum as sum_\n",
        "import time\n",
        "import random\n",
        "import requests\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"NetflixGenreAnalysis\").getOrCreate()\n",
        "\n",
        "# Function to fetch stock price with retry and User-Agent rotation\n",
        "def fetch_stock_price(ticker=\"NFLX\", retries=5, initial_delay=10):\n",
        "    user_agents = [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15',\n",
        "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'\n",
        "    ]\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            # Create a session with random User-Agent\n",
        "            session = requests.Session()\n",
        "            session.headers.update({'User-Agent': random.choice(user_agents)})\n",
        "            # Fetch stock data\n",
        "            stock_df = yf.download(ticker, period=\"5d\", interval=\"1d\", session=session)\n",
        "            if stock_df.empty:\n",
        "                print(f\"⚠️ Empty stock data for {ticker}!\")\n",
        "                raise ValueError(\"Empty DataFrame\")\n",
        "            if stock_df.columns.nlevels > 1:\n",
        "                stock_df.columns = stock_df.columns.get_level_values(0)\n",
        "            close_prices = stock_df['Close']\n",
        "            avg_close_price = close_prices.mean()\n",
        "            return stock_df, avg_close_price\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1}/{retries} failed for {ticker}: {e}\")\n",
        "            if attempt < retries - 1:\n",
        "                delay = initial_delay * (2 ** attempt)  # Exponential backoff: 10s, 20s, 40s, 80s, 160s\n",
        "                print(f\"Retrying after {delay} seconds...\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                print(f\"⚠️ Failed to fetch stock data for {ticker} after {retries} attempts.\")\n",
        "                return None, None\n",
        "\n",
        "# Function to fetch trend interest for multiple genres\n",
        "def fetch_trend_interest(genres):\n",
        "    try:\n",
        "        pytrends = TrendReq(hl='en-US', tz=360)\n",
        "        trend_data = []\n",
        "        for genre in genres:\n",
        "            keyword = f\"Netflix {genre}\"\n",
        "            pytrends.build_payload(kw_list=[keyword], timeframe=\"now 7-d\")\n",
        "            time.sleep(2)  # Avoid PyTrends rate limits\n",
        "            trend_df = pytrends.interest_over_time()\n",
        "            if trend_df.empty:\n",
        "                print(f\"⚠️ Trend data for {keyword} is empty!\")\n",
        "                avg_interest = 10.0  # Fallback value\n",
        "            else:\n",
        "                avg_interest = trend_df[keyword].mean()\n",
        "            trend_data.append({\"genre\": genre, \"avg_interest\": avg_interest})\n",
        "        return pd.DataFrame(trend_data)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error fetching trend data: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Define genres and audience mapping\n",
        "genres = [\"Comedy\", \"Thriller\", \"Drama\", \"Action\"]\n",
        "genre_audience_map = {\n",
        "    \"Comedy\": \"Youth\",\n",
        "    \"Thriller\": \"Youth\",\n",
        "    \"Drama\": \"Adult\",\n",
        "    \"Action\": \"Adult\"\n",
        "}\n",
        "\n",
        "# Pause to avoid cumulative rate limits\n",
        "print(\"Pausing for 60 seconds to avoid Yahoo Finance rate limits...\")\n",
        "time.sleep(60)\n",
        "\n",
        "# Fetch stock price\n",
        "stock_df, avg_price = fetch_stock_price()\n",
        "if avg_price is None:\n",
        "    print(\"⚠️ Using fallback avg_price due to fetch failure.\")\n",
        "    avg_price = 436.22  # Fallback to align with your example\n",
        "\n",
        "# Fetch trend interest for each genre\n",
        "trend_df_pandas = fetch_trend_interest(genres)\n",
        "\n",
        "# Create base_df with genre and audience\n",
        "base_df_data = [(genre, genre_audience_map[genre]) for genre in genres]\n",
        "base_df = spark.createDataFrame(base_df_data, [\"genre\", \"audience\"])\n",
        "\n",
        "# Convert trend_df_pandas to PySpark DataFrame\n",
        "if not trend_df_pandas.empty:\n",
        "    trend_df = spark.createDataFrame(trend_df_pandas)\n",
        "else:\n",
        "    print(\"⚠️ No trend data available!\")\n",
        "    trend_df = spark.createDataFrame([(genre, 10.0) for genre in genres], [\"genre\", \"avg_interest\"])\n",
        "\n",
        "# Join base_df with trend_df\n",
        "df = base_df.join(trend_df, \"genre\", \"left\")\n",
        "\n",
        "# Check if data is valid\n",
        "if avg_price == 0.0 or df.filter(col(\"avg_interest\") > 0).count() == 0:\n",
        "    print(\"⚠️ Invalid data, please check the data fetching functions.\")\n",
        "else:\n",
        "    # Calculate total interest for normalization\n",
        "    total_interest = df.select(sum_(col(\"avg_interest\")).alias(\"total_interest\")).collect()[0][\"total_interest\"]\n",
        "\n",
        "    # Add weight based on avg_interest\n",
        "    df = df.withColumn(\"weight\", col(\"avg_interest\") / total_interest)\n",
        "\n",
        "    # Adjust avg_price based on weight\n",
        "    df = df.withColumn(\"avg_price\", lit(avg_price) * col(\"weight\"))\n",
        "\n",
        "    # Add interest_level\n",
        "    df = df.withColumn(\n",
        "        \"interest_level\",\n",
        "        when(col(\"avg_interest\") >= 60, \"High\")\n",
        "        .when(col(\"avg_interest\") >= 50, \"Moderate\")\n",
        "        .otherwise(\"Low\")\n",
        "    )\n",
        "\n",
        "    # Add price_level\n",
        "    df = df.withColumn(\n",
        "        \"price_level\",\n",
        "        when(col(\"avg_price\") >= 400, \"High\")\n",
        "        .when(col(\"avg_price\") >= 300, \"Moderate\")\n",
        "        .otherwise(\"Low\")\n",
        "    )\n",
        "\n",
        "    # Select final columns\n",
        "    df = df.select(\"genre\", \"audience\", \"avg_price\", \"avg_interest\", \"interest_level\", \"price_level\")\n",
        "\n",
        "    # Show Result\n",
        "    print(\"Final PySpark DataFrame:\")\n",
        "    df.show(truncate=False)\n",
        "\n",
        "    # Save to CSV\n",
        "    df_pd = df.toPandas()\n",
        "    df_pd.to_csv(\"netflix_genre_analysis.csv\", index=False)\n",
        "    print(\"✅ CSV saved as 'netflix_genre_analysis.csv'\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT-vdTRz7O1Q",
        "outputId": "3c2b2b1f-c00d-4843-a767-1b3219b97976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pausing for 60 seconds to avoid Yahoo Finance rate limits...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['NFLX']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Empty stock data for NFLX!\n",
            "Attempt 1/5 failed for NFLX: Empty DataFrame\n",
            "Retrying after 10 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['NFLX']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Empty stock data for NFLX!\n",
            "Attempt 2/5 failed for NFLX: Empty DataFrame\n",
            "Retrying after 20 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['NFLX']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Empty stock data for NFLX!\n",
            "Attempt 3/5 failed for NFLX: Empty DataFrame\n",
            "Retrying after 40 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['NFLX']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Empty stock data for NFLX!\n",
            "Attempt 4/5 failed for NFLX: Empty DataFrame\n",
            "Retrying after 80 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['NFLX']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Empty stock data for NFLX!\n",
            "Attempt 5/5 failed for NFLX: Empty DataFrame\n",
            "⚠️ Failed to fetch stock data for NFLX after 5 attempts.\n",
            "⚠️ Using fallback avg_price due to fetch failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.11/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final PySpark DataFrame:\n",
            "+--------+--------+-----------------+-----------------+--------------+-----------+\n",
            "|genre   |audience|avg_price        |avg_interest     |interest_level|price_level|\n",
            "+--------+--------+-----------------+-----------------+--------------+-----------+\n",
            "|Thriller|Youth   |87.7450001604055 |37.10059171597633|Low           |Low        |\n",
            "|Comedy  |Youth   |77.5990471912996 |32.81065088757396|Low           |Low        |\n",
            "|Drama   |Adult   |145.6818902184723|61.59763313609467|High          |Low        |\n",
            "|Action  |Adult   |125.1940624298226|52.93491124260355|Moderate      |Low        |\n",
            "+--------+--------+-----------------+-----------------+--------------+-----------+\n",
            "\n",
            "✅ CSV saved as 'netflix_genre_analysis.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bhz8dURA7QWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o9S-2e7QqSRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zRxw5jucqSSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S8Pzv0OPqSWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mbkc58CsqOW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B9ElCeDXqOZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AInvNuh-qObk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project"
      ],
      "metadata": {
        "id": "I94NqKyDqP06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf, when, lit\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, DateType\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from textblob import TextBlob\n",
        "import requests\n",
        "import yfinance as yf\n",
        "from datetime import datetime, date, timedelta\n",
        "import random\n",
        "\n",
        "# --- Initialize Spark ---\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NewsStockAnalysisPrediction\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# --- Analysis Functions ---\n",
        "\n",
        "def get_sentiment(text):\n",
        "    if not text or text == 'No headline':\n",
        "        return 'Unknown'\n",
        "    polarity = TextBlob(text).sentiment.polarity\n",
        "    if polarity > 0.2:\n",
        "        return 'Positive'\n",
        "    elif polarity < -0.2:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "def infer_genre(text):\n",
        "    if not text or text == 'No headline':\n",
        "        return 'Unknown'\n",
        "    if 'thriller' in text.lower(): return 'Thriller'\n",
        "    elif 'drama' in text.lower(): return 'Drama'\n",
        "    elif 'comedy' in text.lower(): return 'Comedy'\n",
        "    elif 'romance' in text.lower(): return 'Romance'\n",
        "    else: return 'Other'\n",
        "\n",
        "def infer_audience(text):\n",
        "    if not text or text == 'No headline':\n",
        "        return 'Unknown'\n",
        "    if any(x in text.lower() for x in ['teen', 'gen z', 'youth']): return 'Youth'\n",
        "    elif 'adult' in text.lower(): return 'Adults'\n",
        "    else: return 'General'\n",
        "\n",
        "def calculate_impact(sentiment, open_price, close_price):\n",
        "    if sentiment == 'Unknown' or open_price is None or close_price is None:\n",
        "        return 'Unknown'\n",
        "    change = (close_price - open_price) / open_price * 100\n",
        "    if sentiment == 'Positive' and change > 0:\n",
        "        return 'Positive'\n",
        "    elif sentiment == 'Negative' and change < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Register UDFs\n",
        "sentiment_udf = udf(get_sentiment, StringType())\n",
        "genre_udf = udf(infer_genre, StringType())\n",
        "audience_udf = udf(infer_audience, StringType())\n",
        "impact_udf = udf(calculate_impact, StringType())\n",
        "\n",
        "# --- Data Fetching Functions ---\n",
        "\n",
        "def is_weekend(date_obj):\n",
        "    return date_obj.weekday() >= 5\n",
        "\n",
        "def fetch_news_by_date(api_key, date_str):\n",
        "    url = f\"https://newsapi.org/v2/everything?q=Netflix&from={date_str}&to={date_str}&language=en&sortBy=publishedAt&apiKey={api_key}\"\n",
        "    try:\n",
        "        print(f\"Fetching news for {date_str}\")\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        articles = response.json().get('articles', [])\n",
        "        if not articles:\n",
        "            print(f\"No articles found for {date_str}\")\n",
        "            return None, None\n",
        "        article = articles[0]\n",
        "        return article['title'] or 'No headline', article['publishedAt']\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching news for {date_str}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# def mock_stock_price(date_str):\n",
        "#     print(f\"Using mock stock data for {date_str}\")\n",
        "#     open_price = 600.00 + random.uniform(-10, 10)\n",
        "#     close_price = open_price + random.uniform(-5, 5)\n",
        "#     return round(open_price, 2), round(close_price, 2), datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
        "\n",
        "def fetch_stock_price_by_date(ticker='NFLX', date_str='2025-05-01', use_mock=False):\n",
        "    target_date = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
        "    # if use_mock:\n",
        "    #     return mock_stock_price(date_str)\n",
        "    try:\n",
        "        stock = yf.Ticker(ticker)\n",
        "        df = stock.history(start=date_str, end=(target_date + timedelta(days=1)).strftime(\"%Y-%m-%d\"))\n",
        "        if df.empty:\n",
        "            print(f\"No stock data for {ticker} on {date_str}\")\n",
        "            return None, None, None\n",
        "        stock_date = df.index[0].date()\n",
        "        if stock_date != target_date:\n",
        "            print(f\"Stock data date {stock_date} does not match {target_date}\")\n",
        "            return None, None, None\n",
        "        return round(df['Open'].iloc[0], 2), round(df['Close'].iloc[0], 2), stock_date\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching stock data for {date_str}: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "# --- Prediction Function ---\n",
        "\n",
        "def train_stock_prediction_model(historical_data):\n",
        "    # Prepare features: sentiment score, previous day's close, open\n",
        "    data = historical_data.withColumn(\"sentiment_score\",\n",
        "        when(col(\"sentiment\") == \"Positive\", 1.0)\n",
        "        .when(col(\"sentiment\") == \"Negative\", -1.0)\n",
        "        .otherwise(0.0))\n",
        "\n",
        "    # Create feature vector\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=[\"sentiment_score\", \"price_close\", \"price_open\"],\n",
        "        outputCol=\"features\",\n",
        "        handleInvalid=\"skip\"\n",
        "    )\n",
        "\n",
        "    # Shift data to predict next day's close\n",
        "    from pyspark.sql.window import Window\n",
        "    from pyspark.sql.functions import lag\n",
        "    w = Window.orderBy(\"date\")\n",
        "    data = data.withColumn(\"next_close\", lag(\"price_close\", -1).over(w))\n",
        "\n",
        "    # Drop rows with null values\n",
        "    data = data.dropna(subset=[\"next_close\", \"price_close\", \"price_open\", \"sentiment_score\"])\n",
        "\n",
        "    # Assemble features\n",
        "    data = assembler.transform(data)\n",
        "\n",
        "    # Train Linear Regression\n",
        "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"next_close\")\n",
        "    model = lr.fit(data)\n",
        "    return model, assembler\n",
        "\n",
        "# --- Main Analysis ---\n",
        "\n",
        "def analyze_news_stock_range(api_key, start_date, end_date, use_mock_stock=False):\n",
        "    # Generate date range\n",
        "    start_date_obj = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
        "    end_date_obj = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
        "    if start_date_obj > end_date_obj:\n",
        "        print(\"Error: Start date must be before end date\")\n",
        "        return None\n",
        "\n",
        "    # Collect data\n",
        "    data = []\n",
        "    current_date = start_date_obj\n",
        "    while current_date <= end_date_obj:\n",
        "        date_str = current_date.strftime(\"%Y-%m-%d\")\n",
        "        print(f\"\\nProcessing date: {date_str}\")\n",
        "\n",
        "        # Check future or weekend\n",
        "        is_future = current_date > datetime.now().date()\n",
        "        is_weekend_day = is_weekend(current_date)\n",
        "\n",
        "        if is_future and not use_mock_stock:\n",
        "            data.append((date_str, \"Future date\", None, None, None, None, \"Future date\"))\n",
        "            current_date += timedelta(days=1)\n",
        "            continue\n",
        "        if is_weekend_day and not use_mock_stock:\n",
        "            data.append((date_str, \"Non-trading day\", None, None, None, None, \"Weekend date\"))\n",
        "            current_date += timedelta(days=1)\n",
        "            continue\n",
        "\n",
        "        # Fetch news\n",
        "        headline, published_at = fetch_news_by_date(api_key, date_str)\n",
        "        if not headline:\n",
        "            data.append((date_str, \"No news found\", None, None, None, None, \"No news found\"))\n",
        "            current_date += timedelta(days=1)\n",
        "            continue\n",
        "\n",
        "        # Parse news date\n",
        "        try:\n",
        "            news_dt = datetime.fromisoformat(published_at.replace(\"Z\", \"+00:00\"))\n",
        "            news_date = news_dt.date()\n",
        "            if news_date != current_date:\n",
        "                print(f\"News date {news_date} does not match {current_date}\")\n",
        "                data.append((date_str, headline, published_at, None, None, None, \"News date mismatch\"))\n",
        "                current_date += timedelta(days=1)\n",
        "                continue\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing publishedAt: {e}\")\n",
        "            data.append((date_str, headline, published_at, None, None, None, \"Date parsing error\"))\n",
        "            current_date += timedelta(days=1)\n",
        "            continue\n",
        "\n",
        "        # Fetch stock\n",
        "        open_price, close_price, stock_date = fetch_stock_price_by_date('NFLX', date_str, use_mock=use_mock_stock)\n",
        "        if stock_date is None or stock_date != current_date:\n",
        "            print(f\"Stock date {stock_date} does not match {current_date}\")\n",
        "            data.append((date_str, headline, published_at, None, None, None, \"Stock date mismatch or no stock data\"))\n",
        "            current_date += timedelta(days=1)\n",
        "            continue\n",
        "\n",
        "        data.append((date_str, headline, published_at, open_price, close_price, stock_date, None))\n",
        "        current_date += timedelta(days=1)\n",
        "\n",
        "    # Log data for debugging\n",
        "    # print(\"Collected data:\", data)\n",
        "\n",
        "    # Define explicit schema\n",
        "    schema = StructType([\n",
        "        StructField(\"date\", StringType(), True),\n",
        "        StructField(\"headline\", StringType(), True),\n",
        "        StructField(\"publishedAt\", StringType(), True),\n",
        "        StructField(\"price_open\", DoubleType(), True),\n",
        "        StructField(\"price_close\", DoubleType(), True),\n",
        "        StructField(\"stock_date\", DateType(), True),\n",
        "        StructField(\"impact\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    # Create Spark DataFrame\n",
        "    try:\n",
        "        df = spark.createDataFrame(data, schema)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating DataFrame: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Apply analysis\n",
        "    df = df.withColumn(\"sentiment\", sentiment_udf(col(\"headline\"))) \\\n",
        "           .withColumn(\"genre\", genre_udf(col(\"headline\"))) \\\n",
        "           .withColumn(\"audience\", audience_udf(col(\"headline\"))) \\\n",
        "           .withColumn(\"impact\",\n",
        "                       when(col(\"impact\").isNull(),\n",
        "                            impact_udf(col(\"sentiment\"), col(\"price_open\"), col(\"price_close\"))\n",
        "                           ).otherwise(col(\"impact\")))\n",
        "\n",
        "    # Select final columns\n",
        "    df = df.select(\"date\", \"headline\", \"publishedAt\", \"genre\", \"audience\", \"sentiment\", \"price_close\", \"impact\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# --- Fetch Historical Data for Training ---\n",
        "\n",
        "def fetch_historical_data(ticker='NFLX', start_date=\"2024-04-01\", end_date=\"2024-04-30\", news_api_key=None):\n",
        "    # Fetch stock data\n",
        "    stock = yf.Ticker(ticker)\n",
        "    df = stock.history(start=start_date, end=end_date)\n",
        "    stock_data = [\n",
        "        (row.name.strftime(\"%Y-%m-%d\"), row.Open, row.Close, row.name.date(), \"No headline\", None)\n",
        "        for row in df.itertuples()\n",
        "    ]\n",
        "\n",
        "    # Fetch news for sentiment\n",
        "    current_date = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
        "    end_date_obj = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
        "    while current_date <= end_date_obj:\n",
        "        date_str = current_date.strftime(\"%Y-%m-%d\")\n",
        "        headline, published_at = fetch_news_by_date(news_api_key, date_str)\n",
        "        if headline:\n",
        "            stock_data = [\n",
        "                (d, o, c, sd, headline, published_at) if d == date_str else (d, o, c, sd, h, p)\n",
        "                for d, o, c, sd, h, p in stock_data\n",
        "            ]\n",
        "        current_date += timedelta(days=1)\n",
        "\n",
        "    # Define schema for historical data\n",
        "    schema = StructType([\n",
        "        StructField(\"date\", StringType(), True),\n",
        "        StructField(\"price_open\", DoubleType(), True),\n",
        "        StructField(\"price_close\", DoubleType(), True),\n",
        "        StructField(\"stock_date\", DateType(), True),\n",
        "        StructField(\"headline\", StringType(), True),\n",
        "        StructField(\"publishedAt\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    # Create Spark DataFrame\n",
        "    stock_df = spark.createDataFrame(stock_data, schema)\n",
        "\n",
        "    # Apply sentiment analysis\n",
        "    stock_df = stock_df.withColumn(\"sentiment\", sentiment_udf(col(\"headline\")))\n",
        "\n",
        "    return stock_df\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "API_KEY = \"4c5f88c03cba44bf95380e7ee2cb7002\"  # Replace with your NewsAPI key\n",
        "start_date = \"2025-05-01\"\n",
        "end_date = \"2025-05-10\"\n",
        "\n",
        "# Analyze range\n",
        "analysis_df = analyze_news_stock_range(API_KEY, start_date, end_date, use_mock_stock=True)\n",
        "if analysis_df is not None:\n",
        "    analysis_df.show(truncate=False)\n",
        "else:\n",
        "    print(\"Failed to create analysis DataFrame\")\n",
        "\n",
        "# Train prediction model\n",
        "historical_df = fetch_historical_data('NFLX', \"2024-04-01\", \"2024-04-30\", API_KEY)\n",
        "model, assembler = train_stock_prediction_model(historical_df)\n",
        "\n",
        "# Predict for 2025-05-11\n",
        "latest_data = analysis_df.filter(col(\"date\") == \"2025-05-10\") \\\n",
        "    .select(\"price_close\", \"price_open\", \"sentiment\") \\\n",
        "    .withColumn(\"sentiment_score\",\n",
        "                when(col(\"sentiment\") == \"Positive\", 1.0)\n",
        "                .when(col(\"sentiment\") == \"Negative\", -1.0)\n",
        "                .otherwise(0.0)) \\\n",
        "    .select(\"price_close\", \"price_open\", \"sentiment_score\")\n",
        "\n",
        "if latest_data.count() > 0:\n",
        "    latest_features = assembler.transform(latest_data)\n",
        "    prediction = model.transform(latest_features)\n",
        "    predicted_price = prediction.select(\"prediction\").collect()[0][0]\n",
        "    print(f\"\\nPredicted NFLX closing price for 2025-05-11: ${predicted_price:.2f}\")\n",
        "else:\n",
        "    print(\"\\nNo valid data for 2025-05-10 to make a prediction\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sboYI2rqqOfA",
        "outputId": "b7598b09-b9f7-4454-cbf3-a9cafa5ce8bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing date: 2025-05-01\n",
            "Fetching news for 2025-05-01\n",
            "\n",
            "Processing date: 2025-05-02\n",
            "Fetching news for 2025-05-02\n",
            "\n",
            "Processing date: 2025-05-03\n",
            "Fetching news for 2025-05-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:$NFLX: possibly delisted; no price data found  (1d 2025-05-03 -> 2025-05-04)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No stock data for NFLX on 2025-05-03\n",
            "Stock date None does not match 2025-05-03\n",
            "\n",
            "Processing date: 2025-05-04\n",
            "Fetching news for 2025-05-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:$NFLX: possibly delisted; no price data found  (1d 2025-05-04 -> 2025-05-05)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No stock data for NFLX on 2025-05-04\n",
            "Stock date None does not match 2025-05-04\n",
            "\n",
            "Processing date: 2025-05-05\n",
            "Fetching news for 2025-05-05\n",
            "\n",
            "Processing date: 2025-05-06\n",
            "Fetching news for 2025-05-06\n",
            "\n",
            "Processing date: 2025-05-07\n",
            "Fetching news for 2025-05-07\n",
            "\n",
            "Processing date: 2025-05-08\n",
            "Fetching news for 2025-05-08\n",
            "\n",
            "Processing date: 2025-05-09\n",
            "Fetching news for 2025-05-09\n",
            "\n",
            "Processing date: 2025-05-10\n",
            "Fetching news for 2025-05-10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:$NFLX: possibly delisted; no price data found  (1d 2025-05-10 -> 2025-05-11)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No stock data for NFLX on 2025-05-10\n",
            "Stock date None does not match 2025-05-10\n",
            "Error creating DataFrame: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `1122.52` in type `float64`.\n",
            "Failed to create analysis DataFrame\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Pandas' object has no attribute 'name'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c51e198248af>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;31m# Train prediction model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m \u001b[0mhistorical_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_historical_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NFLX'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2024-04-01\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2024-04-30\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAPI_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_stock_prediction_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistorical_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c51e198248af>\u001b[0m in \u001b[0;36mfetch_historical_data\u001b[0;34m(ticker, start_date, end_date, news_api_key)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0mstock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTicker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     stock_data = [\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No headline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c51e198248af>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     stock_data = [\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No headline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ]\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Pandas' object has no attribute 'name'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf, when\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# --- Initialize Spark ---\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NewsStockPrediction\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# --- Analysis Functions ---\n",
        "\n",
        "def get_sentiment(text):\n",
        "    if not text or text == 'No headline':\n",
        "        return 'Unknown'\n",
        "    polarity = TextBlob(text).sentiment.polarity\n",
        "    if polarity > 0.2:\n",
        "        return 'Positive'\n",
        "    elif polarity < -0.2:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Register UDF\n",
        "sentiment_udf = udf(get_sentiment, StringType())\n",
        "\n",
        "# --- Data Preparation Functions ---\n",
        "\n",
        "def is_weekend(date_obj):\n",
        "    return date_obj.weekday() >= 5\n",
        "\n",
        "def mock_news_by_date(date_str):\n",
        "    \"\"\"Simulate news headlines.\"\"\"\n",
        "    headlines = [\n",
        "        \"Netflix Releases Blockbuster Thriller Series\",\n",
        "        \"Netflix Announces New Comedy Special\",\n",
        "        \"Netflix Stock Surges on Strong Earnings\",\n",
        "        \"Netflix Faces Criticism Over Price Hike\",\n",
        "        \"Netflix Partners with Major Studio\",\n",
        "        \"Netflix Expands Youth Programming\"\n",
        "    ]\n",
        "    published_at = f\"{date_str}T{random.randint(8, 18):02d}:00:00Z\"\n",
        "    headline = random.choice(headlines)\n",
        "    print(f\"Mock news for {date_str}: {headline}\")\n",
        "    return headline, published_at\n",
        "\n",
        "def fetch_news_by_date(api_key, date_str, max_retries=3):\n",
        "    \"\"\"Fetch real news from NewsAPI.\"\"\"\n",
        "    url = f\"https://newsapi.org/v2/everything?q=Netflix&from={date_str}&to={date_str}&language=en&sortBy=publishedAt&apiKey={api_key}\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"Fetching news for {date_str}, attempt {attempt + 1}\")\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            articles = response.json().get('articles', [])\n",
        "            if not articles:\n",
        "                print(f\"No articles found for {date_str}\")\n",
        "                return None, None\n",
        "            article = articles[0]\n",
        "            return article['title'] or 'No headline', article['publishedAt']\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Error fetching news for {date_str}: {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)\n",
        "            else:\n",
        "                return None, None\n",
        "\n",
        "# --- Cache Functions ---\n",
        "\n",
        "def cache_data(data, filename):\n",
        "    if not data:\n",
        "        print(f\"No data to cache in {filename}\")\n",
        "        return\n",
        "    try:\n",
        "        pd.DataFrame(data).to_csv(filename, index=False)\n",
        "        print(f\"Cached data to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error caching data to {filename}: {e}\")\n",
        "\n",
        "def load_cached_data(filename):\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"No cache file found: {filename}\")\n",
        "        return None\n",
        "    try:\n",
        "        df = pd.read_csv(filename)\n",
        "        if df.empty or df.columns.empty:\n",
        "            print(f\"Cache file {filename} is empty or invalid\")\n",
        "            return None\n",
        "        data = df.to_dict('records')\n",
        "        for row in data:\n",
        "            if pd.notnull(row.get('stock_date')):\n",
        "                row['stock_date'] = pd.to_datetime(row['stock_date']).date()\n",
        "        print(f\"Loaded cached data from {filename}\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading cache file {filename}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Fetch Historical Data ---\n",
        "\n",
        "def fetch_historical_data(start_date=\"2025-04-01\", end_date=\"2025-04-29\", news_api_key=None):\n",
        "    cache_file = \"historical_cache.csv\"\n",
        "    cached_data = load_cached_data(cache_file)\n",
        "    if cached_data:\n",
        "        stock_data = cached_data\n",
        "    else:\n",
        "        # Provided historical stock data\n",
        "        stock_data_raw = [\n",
        "            {\"Date\": \"2025-04-01\", \"Open\": 927.50, \"Close\": 928.38},\n",
        "            {\"Date\": \"2025-04-02\", \"Open\": 923.00, \"Close\": 935.52},\n",
        "            {\"Date\": \"2025-04-03\", \"Open\": 901.80, \"Close\": 917.05},\n",
        "            {\"Date\": \"2025-04-04\", \"Open\": 896.50, \"Close\": 855.86},\n",
        "            {\"Date\": \"2025-04-07\", \"Open\": 827.85, \"Close\": 867.83},\n",
        "            {\"Date\": \"2025-04-08\", \"Open\": 912.44, \"Close\": 870.40},\n",
        "            {\"Date\": \"2025-04-09\", \"Open\": 855.93, \"Close\": 945.47},\n",
        "            {\"Date\": \"2025-04-10\", \"Open\": 931.94, \"Close\": 921.17},\n",
        "            {\"Date\": \"2025-04-11\", \"Open\": 920.00, \"Close\": 918.29},\n",
        "            {\"Date\": \"2025-04-14\", \"Open\": 932.70, \"Close\": 931.28},\n",
        "            {\"Date\": \"2025-04-15\", \"Open\": 950.00, \"Close\": 976.28},\n",
        "            {\"Date\": \"2025-04-16\", \"Open\": 976.28, \"Close\": 961.63},\n",
        "            {\"Date\": \"2025-04-17\", \"Open\": 969.00, \"Close\": 973.03},\n",
        "            {\"Date\": \"2025-04-21\", \"Open\": 984.40, \"Close\": 987.91},\n",
        "            {\"Date\": \"2025-04-22\", \"Open\": 1005.30, \"Close\": 1040.34},\n",
        "            {\"Date\": \"2025-04-23\", \"Open\": 1047.22, \"Close\": 1049.59},\n",
        "            {\"Date\": \"2025-04-24\", \"Open\": 1048.00, \"Close\": 1096.87},\n",
        "            {\"Date\": \"2025-04-25\", \"Open\": 1097.04, \"Close\": 1101.53},\n",
        "            {\"Date\": \"2025-04-28\", \"Open\": 1100.00, \"Close\": 1110.38},\n",
        "            {\"Date\": \"2025-04-29\", \"Open\": 1103.92, \"Close\": 1125.64}\n",
        "        ]\n",
        "        stock_data = []\n",
        "        start_date_obj = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
        "        end_date_obj = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
        "        current_date = start_date_obj\n",
        "        while current_date <= end_date_obj:\n",
        "            date_str = current_date.strftime(\"%Y-%m-%d\")\n",
        "            if is_weekend(current_date):\n",
        "                stock_data.append((date_str, None, None, None, \"Non-trading day\", None))\n",
        "                current_date += timedelta(days=1)\n",
        "                continue\n",
        "            # Find stock data for the date\n",
        "            stock_entry = next((entry for entry in stock_data_raw if entry[\"Date\"] == date_str), None)\n",
        "            if stock_entry:\n",
        "                open_price = round(float(stock_entry[\"Open\"]), 2)\n",
        "                close_price = round(float(stock_entry[\"Close\"]), 2)\n",
        "                stock_date = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
        "                # Try NewsAPI for April 12, else use mock news\n",
        "                if date_str == \"2025-04-12\" and news_api_key:\n",
        "                    headline, published_at = fetch_news_by_date(news_api_key, date_str)\n",
        "                    if not headline:\n",
        "                        print(f\"Falling back to mock news for {date_str}\")\n",
        "                        headline, published_at = mock_news_by_date(date_str)\n",
        "                else:\n",
        "                    headline, published_at = mock_news_by_date(date_str)\n",
        "                stock_data.append((date_str, open_price, close_price, stock_date, headline or \"No headline\", published_at))\n",
        "            else:\n",
        "                stock_data.append((date_str, None, None, None, \"No data\", None))\n",
        "            current_date += timedelta(days=1)\n",
        "        # Cache data\n",
        "        cache_data(stock_data, cache_file)\n",
        "    # Define schema\n",
        "    schema = StructType([\n",
        "        StructField(\"date\", StringType(), True),\n",
        "        StructField(\"price_open\", DoubleType(), True),\n",
        "        StructField(\"price_close\", DoubleType(), True),\n",
        "        StructField(\"stock_date\", DateType(), True),\n",
        "        StructField(\"headline\", StringType(), True),\n",
        "        StructField(\"publishedAt\", StringType(), True)\n",
        "    ])\n",
        "    # Create Spark DataFrame\n",
        "    try:\n",
        "        stock_df = spark.createDataFrame(stock_data, schema)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating historical DataFrame: {e}\")\n",
        "        return None\n",
        "    # Apply sentiment analysis\n",
        "    stock_df = stock_df.withColumn(\"sentiment\", sentiment_udf(col(\"headline\")))\n",
        "    return stock_df\n",
        "\n",
        "# --- Prediction Function ---\n",
        "\n",
        "def train_stock_prediction_model(historical_data):\n",
        "    if historical_data is None:\n",
        "        print(\"No historical data to train model\")\n",
        "        return None, None\n",
        "    # Prepare features: sentiment score, previous day's close, open\n",
        "    data = historical_data.withColumn(\"sentiment_score\",\n",
        "        when(col(\"sentiment\") == \"Positive\", 1.0)\n",
        "        .when(col(\"sentiment\") == \"Negative\", -1.0)\n",
        "        .otherwise(0.0))\n",
        "    # Create feature vector\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=[\"sentiment_score\", \"price_close\", \"price_open\"],\n",
        "        outputCol=\"features\",\n",
        "        handleInvalid=\"skip\"\n",
        "    )\n",
        "    # Shift data to predict next day's close\n",
        "    from pyspark.sql.window import Window\n",
        "    from pyspark.sql.functions import lag\n",
        "    w = Window.orderBy(\"date\")\n",
        "    data = data.withColumn(\"next_close\", lag(\"price_close\", -1).over(w))\n",
        "    # Drop rows with null values\n",
        "    data = data.dropna(subset=[\"next_close\", \"price_close\", \"price_open\", \"sentiment_score\"])\n",
        "    # Check if data is empty\n",
        "    if data.count() == 0:\n",
        "        print(\"No valid data for training after cleaning\")\n",
        "        return None, None\n",
        "    # Assemble features\n",
        "    data = assembler.transform(data)\n",
        "    # Train Linear Regression\n",
        "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"next_close\")\n",
        "    model = lr.fit(data)\n",
        "    return model, assembler\n",
        "\n",
        "# --- Analyze Patterns ---\n",
        "\n",
        "def analyze_sentiment_patterns(historical_df):\n",
        "    \"\"\"Analyze correlation between sentiment and price changes.\"\"\"\n",
        "    if historical_df is None:\n",
        "        return None\n",
        "    # Calculate daily price change\n",
        "    df = historical_df.withColumn(\"price_change\",\n",
        "        (col(\"price_close\") - col(\"price_open\")) / col(\"price_open\") * 100)\n",
        "    # Filter trading days\n",
        "    df = df.filter(~col(\"headline\").isin([\"Non-trading day\", \"No data\"]))\n",
        "    # Group by sentiment\n",
        "    sentiment_stats = df.groupBy(\"sentiment\").agg(\n",
        "        {\"price_change\": \"avg\", \"date\": \"count\"}\n",
        "    ).withColumnRenamed(\"avg(price_change)\", \"avg_price_change\") \\\n",
        "     .withColumnRenamed(\"count(date)\", \"num_days\")\n",
        "    # Calculate correlation: Positive sentiment → price increase\n",
        "    positive_up = df.filter((col(\"sentiment\") == \"Positive\") & (col(\"price_change\") > 0)).count()\n",
        "    positive_total = df.filter(col(\"sentiment\") == \"Positive\").count()\n",
        "    negative_down = df.filter((col(\"sentiment\") == \"Negative\") & (col(\"price_change\") < 0)).count()\n",
        "    negative_total = df.filter(col(\"sentiment\") == \"Negative\").count()\n",
        "    patterns = {\n",
        "        \"sentiment_stats\": sentiment_stats.collect(),\n",
        "        \"positive_up_pct\": (positive_up / positive_total * 100) if positive_total > 0 else 0,\n",
        "        \"negative_down_pct\": (negative_down / negative_total * 100) if negative_total > 0 else 0\n",
        "    }\n",
        "    return patterns\n",
        "\n",
        "# --- Predict Next Trading Day ---\n",
        "\n",
        "def predict_next_trading_day(historical_df, model, assembler, news_api_key, predict_date=\"2025-04-12\"):\n",
        "    \"\"\"Predict closing price for the next trading day after predict_date.\"\"\"\n",
        "    predict_date_obj = datetime.strptime(predict_date, \"%Y-%m-%d\").date()\n",
        "    # Find next trading day\n",
        "    next_trading_date = predict_date_obj\n",
        "    while is_weekend(next_trading_date) or next_trading_date > datetime.strptime(\"2025-04-29\", \"%Y-%m-%d\").date():\n",
        "        next_trading_date += timedelta(days=1)\n",
        "    next_trading_date_str = next_trading_date.strftime(\"%Y-%m-%d\")\n",
        "    print(f\"Predicting for next trading day: {next_trading_date_str}\")\n",
        "    # Get news for predict_date\n",
        "    headline, published_at = fetch_news_by_date(news_api_key, predict_date) if news_api_key else (None, None)\n",
        "    if not headline:\n",
        "        print(f\"Falling back to mock news for {predict_date}\")\n",
        "        headline, published_at = mock_news_by_date(predict_date)\n",
        "    sentiment = get_sentiment(headline)\n",
        "    # Get last trading day's data (before predict_date)\n",
        "    last_trading_date = predict_date_obj - timedelta(days=1)\n",
        "    while is_weekend(last_trading_date) or last_trading_date < datetime.strptime(\"2025-04-01\", \"%Y-%m-%d\").date():\n",
        "        last_trading_date -= timedelta(days=1)\n",
        "    last_trading_date_str = last_trading_date.strftime(\"%Y-%m-%d\")\n",
        "    last_day = historical_df.filter(col(\"date\") == last_trading_date_str) \\\n",
        "        .select(\"price_close\", \"price_open\", \"sentiment\") \\\n",
        "        .collect()\n",
        "    if not last_day:\n",
        "        print(f\"No data for {last_trading_date_str} to make prediction\")\n",
        "        return None, None, None\n",
        "    last_row = last_day[0]\n",
        "    # Create prediction DataFrame\n",
        "    prediction_data = [{\n",
        "        \"price_close\": last_row[\"price_close\"],\n",
        "        \"price_open\": last_row[\"price_open\"],\n",
        "        \"sentiment\": sentiment\n",
        "    }]\n",
        "    schema = StructType([\n",
        "        StructField(\"price_close\", DoubleType(), True),\n",
        "        StructField(\"price_open\", DoubleType(), True),\n",
        "        StructField(\"sentiment\", StringType(), True)\n",
        "    ])\n",
        "    pred_df = spark.createDataFrame(prediction_data, schema)\n",
        "    # Add sentiment score\n",
        "    pred_df = pred_df.withColumn(\"sentiment_score\",\n",
        "        when(col(\"sentiment\") == \"Positive\", 1.0)\n",
        "        .when(col(\"sentiment\") == \"Negative\", -1.0)\n",
        "        .otherwise(0.0)) \\\n",
        "        .select(\"price_close\", \"price_open\", \"sentiment_score\")\n",
        "    # Transform and predict\n",
        "    pred_features = assembler.transform(pred_df)\n",
        "    prediction = model.transform(pred_features)\n",
        "    predicted_price = prediction.select(\"prediction\").collect()[0][0]\n",
        "    return predicted_price, headline, sentiment\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "# Set NewsAPI key (replace with your actual key)\n",
        "API_KEY = \"4c5f88c03cba44bf95380e7ee2cb7002\"\n",
        "\n",
        "# Fetch historical data\n",
        "historical_df = fetch_historical_data(\"2025-04-01\", \"2025-04-29\", API_KEY)\n",
        "if historical_df is None:\n",
        "    print(\"Failed to create historical DataFrame\")\n",
        "else:\n",
        "    # print(highest_data = spark.createDataFrame(stock_data, schema))\n",
        "    print(\"dfdfdfdfdf\")\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Error creating historical DataFrame: {e}\")\n",
        "    #     return None\n",
        "    # Apply sentiment analysis\n",
        "    stock_df = stock_df.withColumn(\"sentiment\", sentiment_udf(col(\"headline\")))\n",
        "    # return stock_df\n",
        "\n",
        "# --- Prediction Function ---\n",
        "\n",
        "def train_stock_prediction_model(historical_data):\n",
        "    if historical_data is None:\n",
        "        print(\"No historical data to train model\")\n",
        "        return None, None\n",
        "    # Prepare features: sentiment score, previous day's close, open\n",
        "    data = historical_data.withColumn(\"sentiment_score\",\n",
        "        when(col(\"sentiment\") == \"Positive\", 1.0)\n",
        "        .when(col(\"sentiment\") == \"Negative\", -1.0)\n",
        "        .otherwise(0.0))\n",
        "    # Create feature vector\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=[\"sentiment_score\", \"price_close\", \"price_open\"],\n",
        "        outputCol=\"features\",\n",
        "        handleInvalid=\"skip\"\n",
        "    )\n",
        "    # Shift data to predict next day's close\n",
        "    from pyspark.sql.window import Window\n",
        "    from pyspark.sql.functions import lag\n",
        "    w = Window.orderBy(\"date\")\n",
        "    data = data.withColumn(\"next_close\", lag(\"price_close\", -1).over(w))\n",
        "    # Drop rows with null values\n",
        "    data = data.dropna(subset=[\"next_close\", \"price_close\", \"price_open\", \"sentiment_score\"])\n",
        "    # Check if data is empty\n",
        "    if data.count() == 0:\n",
        "        print(\"No valid data for training after cleaning\")\n",
        "        return None, None\n",
        "    # Assemble features\n",
        "    data = assembler.transform(data)\n",
        "    # Train Linear Regression\n",
        "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"next_close\")\n",
        "    model = lr.fit(data)\n",
        "    return model, assembler\n",
        "\n",
        "# --- Analyze Patterns ---\n",
        "\n",
        "def analyze_sentiment_patterns(historical_df):\n",
        "    \"\"\"Analyze correlation between sentiment and price changes.\"\"\"\n",
        "    if historical_df is None:\n",
        "        return None\n",
        "    # Calculate daily price change\n",
        "    df = historical_df.withColumn(\"price_change\",\n",
        "        (col(\"price_close\") - col(\"price_open\")) / col(\"price_open\") * 100)\n",
        "    # Filter trading days\n",
        "    df = df.filter(~col(\"headline\").isin([\"Non-trading day\", \"No data\"]))\n",
        "    # Group by sentiment\n",
        "    sentiment_stats = df.groupBy(\"sentiment\").agg(\n",
        "        {\"price_change\": \"avg\", \"date\": \"count\"}\n",
        "    ).withColumnRenamed(\"avg(price_change)\", \"avg_price_change\") \\\n",
        "     .withColumnRenamed(\"count(date)\", \"num_days\")\n",
        "    # Calculate correlation: Positive sentiment → price increase\n",
        "    positive_up = df.filter((col(\"sentiment\") == \"Positive\") & (col(\"price_change\") > 0)).count()\n",
        "    positive_total = df.filter(col(\"sentiment\") == \"Positive\").count()\n",
        "    negative_down = df.filter((col(\"sentiment\") == \"Negative\") & (col(\"price_change\") < 0)).count()\n",
        "    negative_total = df.filter(col(\"sentiment\") == \"Negative\").count()\n",
        "    patterns = {\n",
        "        \"sentiment_stats\": sentiment_stats.collect(),\n",
        "        \"positive_up_pct\": (positive_up / positive_total * 100) if positive_total > 0 else 0,\n",
        "        \"negative_down_pct\": (negative_down / negative_total * 100) if negative_total > 0 else 0\n",
        "    }\n",
        "    return patterns\n",
        "\n",
        "# --- Predict Next Trading Day ---\n",
        "\n",
        "def predict_next_trading_day(historical_df, model, assembler, news_api_key, predict_date=\"2025-04-12\"):\n",
        "    \"\"\"Predict closing price for the next trading day after predict_date.\"\"\"\n",
        "    predict_date_obj = datetime.strptime(predict_date, \"%Y-%m-%d\").date()\n",
        "    # Find next trading day\n",
        "    next_trading_date = predict_date_obj\n",
        "    while is_weekend(next_trading_date) or next_trading_date > datetime.strptime(\"2025-04-29\", \"%Y-%m-%d\").date():\n",
        "        next_trading_date += timedelta(days=1)\n",
        "    next_trading_date_str = next_trading_date.strftime(\"%Y-%m-%d\")\n",
        "    print(f\"Predicting for next trading day: {next_trading_date_str}\")\n",
        "    # Get news for predict_date\n",
        "    headline, published_at = fetch_news_by_date(news_api_key, predict_date) if news_api_key else (None, None)\n",
        "    if not headline:\n",
        "        print(f\"Falling back to mock news for {predict_date}\")\n",
        "        headline, published_at = mock_news_by_date(predict_date)\n",
        "    sentiment = get_sentiment(headline)\n",
        "    # Get last trading day's data (before predict_date)\n",
        "    last_trading_date = predict_date_obj - timedelta(days=1)\n",
        "    while is_weekend(last_trading_date) or last_trading_date < datetime.strptime(\"2025-04-01\", \"%Y-%m-%d\").date():\n",
        "        last_trading_date -= timedelta(days=1)\n",
        "    last_trading_date_str = last_trading_date.strftime(\"%Y-%m-%d\")\n",
        "    last_day = historical_df.filter(col(\"date\") == last_trading_date_str) \\\n",
        "        .select(\"price_close\", \"price_open\", \"sentiment\") \\\n",
        "        .collect()\n",
        "    if not last_day:\n",
        "        print(f\"No data for {last_trading_date_str} to make prediction\")\n",
        "        return None, None, None\n",
        "    last_row = last_day[0]\n",
        "    # Create prediction DataFrame\n",
        "    prediction_data = [{\n",
        "        \"price_close\": last_row[\"price_close\"],\n",
        "        \"price_open\": last_row[\"price_open\"],\n",
        "        \"sentiment\": sentiment\n",
        "    }]\n",
        "    schema = StructType([\n",
        "        StructField(\"price_close\", DoubleType(), True),\n",
        "        StructField(\"price_open\", DoubleType(), True),\n",
        "        StructField(\"sentiment\", StringType(), True)\n",
        "    ])\n",
        "    pred_df = spark.createDataFrame(prediction_data, schema)\n",
        "    # Add sentiment score\n",
        "    pred_df = pred_df.withColumn(\"sentiment_score\",\n",
        "        when(col(\"sentiment\") == \"Positive\", 1.0)\n",
        "        .when(col(\"sentiment\") == \"Negative\", -1.0)\n",
        "        .otherwise(0.0)) \\\n",
        "        .select(\"price_close\", \"price_open\", \"sentiment_score\")\n",
        "    # Transform and predict\n",
        "    pred_features = assembler.transform(pred_df)\n",
        "    prediction = model.transform(pred_features)\n",
        "    predicted_price = prediction.select(\"prediction\").collect()[0][0]\n",
        "    return predicted_price, headline, sentiment\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "# Set NewsAPI key (replace with your actual key)\n",
        "API_KEY = \"4c5f88c03cba44bf95380e7ee2cb7002\"\n",
        "\n",
        "# Fetch historical data\n",
        "historical_df = fetch_historical_data(\"2025-04-01\", \"2025-04-29\", API_KEY)\n",
        "if historical_df is None:\n",
        "    print(\"Failed to create historical DataFrame\")\n",
        "else:\n",
        "    print(\"\\nHistorical Data:\")\n",
        "    historical_df.select(\"date\", \"headline\", \"sentiment\", \"price_open\", \"price_close\").show(truncate=False)\n",
        "\n",
        "    # Train model\n",
        "    model, assembler = train_stock_prediction_model(historical_df)\n",
        "\n",
        "    # Analyze sentiment patterns\n",
        "    patterns = analyze_sentiment_patterns(historical_df)\n",
        "    if patterns:\n",
        "        print(\"\\nSentiment Patterns:\")\n",
        "        for row in patterns[\"sentiment_stats\"]:\n",
        "            print(f\"Sentiment: {row['sentiment']}, Days: {row['num_days']}, Avg Price Change: {row['avg_price_change']:.2f}%\")\n",
        "        print(f\"Positive sentiment → Price increase: {patterns['positive_up_pct']:.2f}% of days\")\n",
        "        print(f\"Negative sentiment → Price decrease: {patterns['negative_down_pct']:.2f}% of days\")\n",
        "\n",
        "    # Predict for April 14, 2025 (next trading day after April 12)\n",
        "    if model is not None and assembler is not None:\n",
        "        predicted_price, headline, sentiment = predict_next_trading_day(historical_df, model, assembler, API_KEY, \"2025-04-12\")\n",
        "        if predicted_price:\n",
        "            print(f\"\\nPrediction for 2025-04-14:\")\n",
        "            print(f\"News headline (2025-04-12): {headline}\")\n",
        "            print(f\"Sentiment: {sentiment}\")\n",
        "            print(f\"Predicted NFLX closing price: ${predicted_price:.2f}\")\n",
        "            # Compare with actual (if available)\n",
        "            actual = historical_df.filter(col(\"date\") == \"2025-04-14\").select(\"price_close\").collect()\n",
        "            if actual:\n",
        "                print(f\"Actual closing price: ${actual[0]['price_close']:.2f}\")\n",
        "        else:\n",
        "            print(\"Cannot predict due to missing data\")\n",
        "    else:\n",
        "        print(\"Failed to train prediction model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "bSlLGfX2qVwy",
        "outputId": "662a2929-e975-4bb4-d914-560a88775f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded cached data from historical_cache.csv\n",
            "dfdfdfdfdf\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'stock_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f0fa352f55f2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;31m#     return None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m# Apply sentiment analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mstock_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstock_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"headline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0;31m# return stock_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stock_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j5h8pnzEtc57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9U8Ztt9r7qhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT Output"
      ],
      "metadata": {
        "id": "AvYYoVnE7rmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# --- Initialize Spark Session ---\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NewsStockAnalysisPrediction\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# --- Sample Stock Data for April 2025 ---\n",
        "data = [\n",
        "    (\"2025-04-01\", 927.50, 932.29, 911.50, 928.38, 3520000),\n",
        "    (\"2025-04-02\", 923.00, 949.55, 916.11, 935.52, 3256900),\n",
        "    (\"2025-04-03\", 901.80, 946.59, 900.47, 917.05, 5864600),\n",
        "    (\"2025-04-04\", 896.50, 906.63, 853.87, 855.86, 6798800),\n",
        "    (\"2025-04-07\", 827.85, 906.74, 821.10, 867.83, 6656800),\n",
        "    (\"2025-04-08\", 912.44, 922.42, 857.70, 870.40, 5625400),\n",
        "    (\"2025-04-09\", 855.93, 951.43, 854.40, 945.47, 7498000),\n",
        "    (\"2025-04-10\", 931.94, 941.24, 894.00, 921.17, 5129800),\n",
        "    (\"2025-04-11\", 920.00, 944.86, 906.68, 918.29, 4073600),\n",
        "    (\"2025-04-14\", 932.70, 948.98, 919.50, 931.28, 4035900),\n",
        "    (\"2025-04-15\", 950.00, 993.45, 948.00, 976.28, 7712000),\n",
        "    (\"2025-04-16\", 976.28, 981.21, 949.17, 961.63, 6066100),\n",
        "    (\"2025-04-17\", 969.00, 984.70, 956.00, 973.03, 8763200),\n",
        "    (\"2025-04-21\", 984.40, 1019.00, 973.05, 987.91, 9775700),\n",
        "    (\"2025-04-22\", 1005.30, 1064.97, 1004.52, 1040.34, 9368900),\n",
        "    (\"2025-04-23\", 1047.22, 1061.25, 1032.00, 1049.59, 6471900),\n",
        "    (\"2025-04-24\", 1048.00, 1101.00, 1047.02, 1096.87, 6381900),\n",
        "    (\"2025-04-25\", 1097.04, 1106.80, 1091.00, 1101.53, 3950000),\n",
        "    (\"2025-04-28\", 1100.00, 1114.00, 1082.62, 1110.38, 3831100),\n",
        "    (\"2025-04-29\", 1103.92, 1127.81, 1095.48, 1125.64, 3777300),\n",
        "]\n",
        "\n",
        "\n",
        "headlines = [\n",
        "    \"Netflix Releases New Thriller Series\",\n",
        "    \"Netflix Stock Surges on New Drama Release\",\n",
        "    \"Netflix Faces Criticism Over Content\",\n",
        "    \"Netflix Announces New Comedy Special\",\n",
        "    \"Netflix Expands Youth Programming\",\n",
        "    \"Netflix Signs Deal with Disney\",\n",
        "    \"Netflix Reports Record Profits\",\n",
        "    \"Netflix Delays Major Release\",\n",
        "    \"Netflix Drops Surprise Documentary\",\n",
        "    \"Netflix Gets Mixed Reviews on New Show\",\n",
        "    \"Netflix Enters Gaming Market\",\n",
        "    \"Netflix Launches Gen Z Content\",\n",
        "    \"Netflix Cancels Popular Series\",\n",
        "    \"Netflix Renews Fan Favorite\",\n",
        "    \"Netflix Faces User Backlash\",\n",
        "    \"Netflix Adds Interactive Show\",\n",
        "    \"Netflix Partners with HBO\",\n",
        "    \"Netflix Cuts Subscription Price\",\n",
        "    \"Netflix Faces Ad Boycott\"\n",
        "]\n",
        "\n",
        "# --- Convert to Pandas ---\n",
        "columns = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
        "pdf = pd.DataFrame(data, columns=columns)\n",
        "pdf[\"Date\"] = pd.to_datetime(pdf[\"Date\"])\n",
        "mock_headlines = (headlines * ((len(pdf) // len(headlines)) + 1))[:len(pdf)]\n",
        "pdf[\"headline\"] = mock_headlines\n",
        "\n",
        "# --- Sentiment Calculation ---\n",
        "def get_sentiment(text):\n",
        "    return TextBlob(text).sentiment.polarity if text else 0.0\n",
        "\n",
        "pdf[\"sentiment\"] = pdf[\"headline\"].apply(get_sentiment)\n",
        "\n",
        "# --- Use Previous Day's Sentiment ---\n",
        "pdf[\"prev_sentiment\"] = pdf[\"sentiment\"].shift(1)\n",
        "pdf = pdf.dropna().reset_index(drop=True)  # Remove first row (no prev sentiment)\n",
        "\n",
        "# --- Convert to Spark DataFrame ---\n",
        "sdf = spark.createDataFrame(pdf)\n",
        "\n",
        "# --- Feature Vector ---\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"Open\", \"High\", \"Low\", \"Volume\", \"prev_sentiment\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "assembled_df = assembler.transform(sdf)\n",
        "\n",
        "# --- Train Model ---\n",
        "train_data = assembled_df.select(\"features\", \"Close\")\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Close\")\n",
        "model = lr.fit(train_data)\n",
        "\n",
        "# --- Predict for Next Day using last available day's stock + sentiment ---\n",
        "last_row = pdf.iloc[-1]\n",
        "\n",
        "features_dict = {\n",
        "    \"Open\": float(last_row[\"Open\"]),\n",
        "    \"High\": float(last_row[\"High\"]),\n",
        "    \"Low\": float(last_row[\"Low\"]),\n",
        "    \"Volume\": float(last_row[\"Volume\"]),\n",
        "    \"prev_sentiment\": float(last_row[\"sentiment\"])\n",
        "}\n",
        "\n",
        "predict_df = spark.createDataFrame([features_dict])\n",
        "predict_vector = assembler.transform(predict_df)\n",
        "prediction = model.transform(predict_vector).collect()[0][\"prediction\"]\n",
        "\n",
        "# --- Output ---\n",
        "tomorrow_date = last_row[\"Date\"].date() + timedelta(days=1)\n",
        "print(f\"\\n📅 Prediction for {tomorrow_date} based on headline of {last_row['Date'].date()}:\")\n",
        "print(f\"📰 Headline: {last_row['headline']}\")\n",
        "print(f\"🧠 Sentiment Score: {last_row['sentiment']:.2f}\")\n",
        "print(f\"📈 Predicted Close Price: {prediction:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7cubYRL7qka",
        "outputId": "b8093108-3ebb-4136-de1e-f39397eb1676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📅 Prediction for 2025-04-30 based on headline of 2025-04-29:\n",
            "📰 Headline: Netflix Releases New Thriller Series\n",
            "🧠 Sentiment Score: 0.14\n",
            "📈 Predicted Close Price: 1121.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DkLsVUA07qn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XL0wp9yd3cLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf, when, lit\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "from datetime import datetime, date, timedelta\n",
        "import random\n",
        "import os\n",
        "\n",
        "# --- Initialize Spark ---\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NewsStockAnalysisPrediction\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# --- Analysis Functions ---\n",
        "\n",
        "def get_sentiment(text):\n",
        "    if not text or text == 'No headline':\n",
        "        return 'Unknown'\n",
        "    polarity = TextBlob(text).sentiment.polarity\n",
        "    if polarity > 0.2:\n",
        "        return 'Positive'\n",
        "    elif polarity < -0.2:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "def infer_genre(text):\n",
        "    if not text or text == 'No headline':\n",
        "        return 'Unknown'\n",
        "    if 'thriller' in text.lower(): return 'Thriller'\n",
        "    elif 'drama' in text.lower(): return 'Drama'\n",
        "    elif 'comedy' in text.lower(): return 'Comedy'\n",
        "    elif 'romance' in text.lower(): return 'Romance'\n",
        "    else: return 'Other'\n",
        "\n",
        "def infer_audience(text):\n",
        "    if not text or text == 'No headline':\n",
        "        return 'Unknown'\n",
        "    if any(x in text.lower() for x in ['teen', 'gen z', 'youth']): return 'Youth'\n",
        "    elif 'adult' in text.lower(): return 'Adults'\n",
        "    else: return 'General'\n",
        "\n",
        "def calculate_impact(sentiment, open_price, close_price):\n",
        "    if sentiment == 'Unknown' or open_price is None or close_price is None:\n",
        "        return 'Unknown'\n",
        "    change = (close_price - open_price) / open_price * 100\n",
        "    if sentiment == 'Positive' and change > 0:\n",
        "        return 'Positive'\n",
        "    elif sentiment == 'Negative' and change < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Register UDFs\n",
        "sentiment_udf = udf(get_sentiment, StringType())\n",
        "genre_udf = udf(infer_genre, StringType())\n",
        "audience_udf = udf(infer_audience, StringType())\n",
        "impact_udf = udf(calculate_impact, StringType())\n",
        "\n",
        "# --- Data Fetching Functions ---\n",
        "\n",
        "def is_weekend(date_obj):\n",
        "    return date_obj.weekday() >= 5\n",
        "\n",
        "def mock_news_by_date(date_str):\n",
        "    \"\"\"Simulate news headlines.\"\"\"\n",
        "    headlines = [\n",
        "        \"Netflix Releases New Thriller Series\",\n",
        "        \"Netflix Announces New Comedy Special\",\n",
        "        \"Netflix Stock Surges on New Drama Release\",\n",
        "        \"Netflix Faces Criticism Over Content\",\n",
        "        \"Netflix Partners for New Series\",\n",
        "        \"Netflix Expands Youth Programming\"\n",
        "    ]\n",
        "    published_at = f\"{date_str}T{random.randint(8, 18):02d}:00:00Z\"\n",
        "    if random.random() < 0.3:  # 30% chance of no news\n",
        "        print(f\"No mock news for {date_str}\")\n",
        "        return None, None\n",
        "    headline = random.choice(headlines)\n",
        "    print(f\"Mock news for {date_str}: {headline}\")\n",
        "    return headline, published_at\n",
        "\n",
        "def mock_stock_price(date_str, sentiment=None):\n",
        "    \"\"\"Simulate stock prices, tied to sentiment.\"\"\"\n",
        "    open_price = 600.00 + random.uniform(-10, 10)\n",
        "    if sentiment == \"Positive\":\n",
        "        close_price = open_price + random.uniform(0, 5)\n",
        "    elif sentiment == \"Negative\":\n",
        "        close_price = open_price + random.uniform(-5, 0)\n",
        "    else:\n",
        "        close_price = open_price + random.uniform(-5, 5)\n",
        "    return round(open_price, 2), round(close_price, 2), datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
        "\n",
        "# --- Cache Functions ---\n",
        "\n",
        "def cache_data(data, filename):\n",
        "    if not data:\n",
        "        print(f\"No data to cache in {filename}\")\n",
        "        return\n",
        "    try:\n",
        "        pd.DataFrame(data).to_csv(filename, index=False)\n",
        "        print(f\"Cached data to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error caching data to {filename}: {e}\")\n",
        "\n",
        "def load_cached_data(filename):\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"No cache file found: {filename}\")\n",
        "        return None\n",
        "    try:\n",
        "        df = pd.read_csv(filename)\n",
        "        if df.empty or df.columns.empty:\n",
        "            print(f\"Cache file {filename} is empty or invalid\")\n",
        "            return None\n",
        "        data = df.to_dict('records')\n",
        "        for row in data:\n",
        "            if pd.notnull(row.get('stock_date')):\n",
        "                row['stock_date'] = pd.to_datetime(row['stock_date']).date()\n",
        "        print(f\"Loaded cached data from {filename}\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading cache file {filename}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Main Analysis ---\n",
        "\n",
        "def analyze_news_stock_range(start_date, end_date):\n",
        "    cache_file = \"news_stock_cache.csv\"\n",
        "    cached_data = load_cached_data(cache_file)\n",
        "    if cached_data:\n",
        "        data = cached_data\n",
        "    else:\n",
        "        # Generate date range\n",
        "        start_date_obj = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
        "        end_date_obj = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
        "        if start_date_obj > end_date_obj:\n",
        "            print(\"Error: Start date must be before end date\")\n",
        "            return None\n",
        "\n",
        "        # Collect data\n",
        "        data = []\n",
        "        current_date = start_date_obj\n",
        "        while current_date <= end_date_obj:\n",
        "            date_str = current_date.strftime(\"%Y-%m-%d\")\n",
        "            print(f\"\\nProcessing date: {date_str}\")\n",
        "\n",
        "            # Check weekend\n",
        "            if is_weekend(current_date):\n",
        "                data.append((date_str, \"Non-trading day\", None, None, None, None, \"Weekend date\"))\n",
        "                current_date += timedelta(days=1)\n",
        "                continue\n",
        "\n",
        "            # Fetch mock news\n",
        "            headline, published_at = mock_news_by_date(date_str)\n",
        "            sentiment = get_sentiment(headline) if headline else \"Unknown\"\n",
        "            if not headline:\n",
        "                data.append((date_str, \"No news found\", None, None, None, None, \"No news found\"))\n",
        "                current_date += timedelta(days=1)\n",
        "                continue\n",
        "\n",
        "            # Parse news date\n",
        "            try:\n",
        "                news_dt = datetime.fromisoformat(published_at.replace(\"Z\", \"+00:00\"))\n",
        "                news_date = news_dt.date()\n",
        "                if news_date != current_date:\n",
        "                    print(f\"News date {news_date} does not match {current_date}\")\n",
        "                    data.append((date_str, headline, published_at, None, None, None, \"News date mismatch\"))\n",
        "                    current_date += timedelta(days=1)\n",
        "                    continue\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing publishedAt: {e}\")\n",
        "                data.append((date_str, headline, published_at, None, None, None, \"Date parsing error\"))\n",
        "                current_date += timedelta(days=1)\n",
        "                continue\n",
        "\n",
        "            # Fetch mock stock\n",
        "            open_price, close_price, stock_date = mock_stock_price(date_str, sentiment)\n",
        "            if stock_date is None or stock_date != current_date:\n",
        "                print(f\"Stock date {stock_date} does not match {current_date}\")\n",
        "                data.append((date_str, headline, published_at, None, None, None, \"Stock date mismatch\"))\n",
        "                current_date += timedelta(days=1)\n",
        "                continue\n",
        "\n",
        "            data.append((date_str, headline, published_at, open_price, close_price, stock_date, None))\n",
        "            current_date += timedelta(days=1)\n",
        "\n",
        "        # Cache data\n",
        "        cache_data(data, cache_file)\n",
        "\n",
        "    # Define explicit schema\n",
        "    schema = StructType([\n",
        "        StructField(\"date\", StringType(), True),\n",
        "        StructField(\"headline\", StringType(), True),\n",
        "        StructField(\"publishedAt\", StringType(), True),\n",
        "        StructField(\"price_open\", DoubleType(), True),\n",
        "        StructField(\"price_close\", DoubleType(), True),\n",
        "        StructField(\"stock_date\", DateType(), True),\n",
        "        StructField(\"impact\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    # Create Spark DataFrame\n",
        "    try:\n",
        "        df = spark.createDataFrame(data, schema)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating DataFrame: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Apply analysis\n",
        "    df = df.withColumn(\"sentiment\", sentiment_udf(col(\"headline\"))) \\\n",
        "           .withColumn(\"genre\", genre_udf(col(\"headline\"))) \\\n",
        "           .withColumn(\"audience\", audience_udf(col(\"headline\"))) \\\n",
        "           .withColumn(\"impact\",\n",
        "                       when(col(\"impact\").isNull(),\n",
        "                            impact_udf(col(\"sentiment\"), col(\"price_open\"), col(\"price_close\"))\n",
        "                           ).otherwise(col(\"impact\")))\n",
        "\n",
        "    # Select final columns\n",
        "    df = df.select(\"date\", \"headline\", \"publishedAt\", \"genre\", \"audience\", \"sentiment\", \"price_close\", \"impact\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# --- Fetch Historical Data for Training ---\n",
        "\n",
        "def fetch_historical_data(ticker='NFLX', start_date=\"2024-04-01\", end_date=\"2024-04-30\"):\n",
        "    cache_file = \"historical_cache.csv\"\n",
        "    cached_data = load_cached_data(cache_file)\n",
        "    if cached_data:\n",
        "        stock_data = cached_data\n",
        "    else:\n",
        "        # Generate date range\n",
        "        start_date_obj = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
        "        end_date_obj = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
        "        stock_data = []\n",
        "        current_date = start_date_obj\n",
        "        while current_date <= end_date_obj:\n",
        "            date_str = current_date.strftime(\"%Y-%m-%d\")\n",
        "            if is_weekend(current_date):\n",
        "                stock_data.append((date_str, None, None, None, \"Non-trading day\", None))\n",
        "                current_date += timedelta(days=1)\n",
        "                continue\n",
        "            # Fetch mock news\n",
        "            headline, published_at = mock_news_by_date(date_str)\n",
        "            sentiment = get_sentiment(headline) if headline else \"Unknown\"\n",
        "            # Fetch mock stock\n",
        "            open_price, close_price, stock_date = mock_stock_price(date_str, sentiment)\n",
        "            stock_data.append((date_str, open_price, close_price, stock_date, headline or \"No headline\", published_at))\n",
        "            current_date += timedelta(days=1)\n",
        "\n",
        "        # Cache data\n",
        "        cache_data(stock_data, cache_file)\n",
        "\n",
        "    # Define schema for historical data\n",
        "    schema = StructType([\n",
        "        StructField(\"date\", StringType(), True),\n",
        "        StructField(\"price_open\", DoubleType(), True),\n",
        "        StructField(\"price_close\", DoubleType(), True),\n",
        "        StructField(\"stock_date\", DateType(), True),\n",
        "        StructField(\"headline\", StringType(), True),\n",
        "        StructField(\"publishedAt\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    # Create Spark DataFrame\n",
        "    try:\n",
        "        stock_df = spark.createDataFrame(stock_data, schema)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating historical DataFrame: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Apply sentiment analysis\n",
        "    stock_df = stock_df.withColumn(\"sentiment\", sentiment_udf(col(\"headline\")))\n",
        "\n",
        "    return stock_df\n",
        "\n",
        "# --- Prediction Function ---\n",
        "\n",
        "def train_stock_prediction_model(historical_data):\n",
        "    if historical_data is None:\n",
        "        print(\"No historical data to train model\")\n",
        "        return None, None\n",
        "\n",
        "    # Prepare features: sentiment score, previous day's close, open\n",
        "    data = historical_data.withColumn(\"sentiment_score\",\n",
        "        when(col(\"sentiment\") == \"Positive\", 1.0)\n",
        "        .when(col(\"sentiment\") == \"Negative\", -1.0)\n",
        "        .otherwise(0.0))\n",
        "\n",
        "    # Create feature vector\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=[\"sentiment_score\", \"price_close\", \"price_open\"],\n",
        "        outputCol=\"features\",\n",
        "        handleInvalid=\"skip\"\n",
        "    )\n",
        "\n",
        "    # Shift data to predict next day's close\n",
        "    from pyspark.sql.window import Window\n",
        "    from pyspark.sql.functions import lag\n",
        "    w = Window.orderBy(\"date\")\n",
        "    data = data.withColumn(\"next_close\", lag(\"price_close\", -1).over(w))\n",
        "\n",
        "    # Drop rows with null values\n",
        "    data = data.dropna(subset=[\"next_close\", \"price_close\", \"price_open\", \"sentiment_score\"])\n",
        "\n",
        "    # Check if data is empty\n",
        "    if data.count() == 0:\n",
        "        print(\"No valid data for training after cleaning\")\n",
        "        return None, None\n",
        "\n",
        "    # Assemble features\n",
        "    data = assembler.transform(data)\n",
        "\n",
        "    # Train Linear Regression\n",
        "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"next_close\")\n",
        "    model = lr.fit(data)\n",
        "    return model, assembler\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "# Analyze range\n",
        "start_date = \"2025-05-01\"\n",
        "end_date = \"2025-05-14\"  # Extended to include current date (2025-05-14)\n",
        "analysis_df = analyze_news_stock_range(start_date, end_date)\n",
        "if analysis_df is not None:\n",
        "    analysis_df.show(truncate=False)\n",
        "else:\n",
        "    print(\"Failed to create analysis DataFrame\")\n",
        "\n",
        "# Train prediction model\n",
        "historical_df = fetch_historical_data('NFLX', \"2024-04-01\", \"2024-04-30\")\n",
        "model, assembler = train_stock_prediction_model(historical_df)\n",
        "\n",
        "# Dynamic prediction based on current date\n",
        "if model is not None and assembler is not None:\n",
        "    # Get current date and previous day\n",
        "    current_date = datetime.now().date()\n",
        "    previous_date = current_date - timedelta(days=1)\n",
        "    previous_date_str = previous_date.strftime(\"%Y-%m-%d\")\n",
        "    tomorrow_date = current_date + timedelta(days=1)\n",
        "\n",
        "    # Fetch previous day's data\n",
        "    previous_data = analysis_df.filter(col(\"date\") == previous_date_str) \\\n",
        "        .select(\"date\", \"headline\", \"sentiment\", \"price_close\", \"price_open\") \\\n",
        "        .withColumn(\"sentiment_score\",\n",
        "                    when(col(\"sentiment\") == \"Positive\", 1.0)\n",
        "                    .when(col(\"sentiment\") == \"Negative\", -1.0)\n",
        "                    .otherwise(0.0)) \\\n",
        "        .select(\"date\", \"headline\", \"sentiment\", \"price_close\", \"price_open\", \"sentiment_score\")\n",
        "\n",
        "    if previous_data.count() > 0:\n",
        "        # Convert to pandas for easier access\n",
        "        previous_row = previous_data.toPandas().iloc[0]\n",
        "\n",
        "        # Print previous day's data\n",
        "        print(f\"\\n📅 Data for {previous_date}:\")\n",
        "        print(f\"📰 Headline: {previous_row['headline']}\")\n",
        "        print(f\"🧠 Sentiment: {previous_row['sentiment']}\")\n",
        "        print(f\"📈 Close Price: ${previous_row['price_close']:.2f}\")\n",
        "\n",
        "        # Prepare features for prediction\n",
        "        latest_features = assembler.transform(previous_data)\n",
        "        prediction = model.transform(latest_features)\n",
        "        predicted_price = prediction.select(\"prediction\").collect()[0][0]\n",
        "\n",
        "        # Print prediction for tomorrow\n",
        "        print(f\"\\n📅 Prediction for {tomorrow_date}:\")\n",
        "        print(f\"📰 Based on Headline from {previous_date}: {previous_row['headline']}\")\n",
        "        print(f\"🧠 Sentiment: {previous_row['sentiment']}\")\n",
        "        print(f\"📈 Predicted Close Price: ${predicted_price:.2f}\")\n",
        "    else:\n",
        "        print(f\"No valid data for {previous_date_str} to make a prediction\")\n",
        "else:\n",
        "    print(\"Failed to train prediction model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cMKsLmio3cO4",
        "outputId": "f6bbb865-705b-475f-d265-f5863cce7993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No cache file found: news_stock_cache.csv\n",
            "\n",
            "Processing date: 2025-05-01\n",
            "No mock news for 2025-05-01\n",
            "\n",
            "Processing date: 2025-05-02\n",
            "Mock news for 2025-05-02: Netflix Announces New Comedy Special\n",
            "\n",
            "Processing date: 2025-05-03\n",
            "\n",
            "Processing date: 2025-05-04\n",
            "\n",
            "Processing date: 2025-05-05\n",
            "Mock news for 2025-05-05: Netflix Releases New Thriller Series\n",
            "\n",
            "Processing date: 2025-05-06\n",
            "Mock news for 2025-05-06: Netflix Partners for New Series\n",
            "\n",
            "Processing date: 2025-05-07\n",
            "Mock news for 2025-05-07: Netflix Stock Surges on New Drama Release\n",
            "\n",
            "Processing date: 2025-05-08\n",
            "No mock news for 2025-05-08\n",
            "\n",
            "Processing date: 2025-05-09\n",
            "Mock news for 2025-05-09: Netflix Expands Youth Programming\n",
            "\n",
            "Processing date: 2025-05-10\n",
            "\n",
            "Processing date: 2025-05-11\n",
            "\n",
            "Processing date: 2025-05-12\n",
            "Mock news for 2025-05-12: Netflix Stock Surges on New Drama Release\n",
            "\n",
            "Processing date: 2025-05-13\n",
            "Mock news for 2025-05-13: Netflix Partners for New Series\n",
            "\n",
            "Processing date: 2025-05-14\n",
            "Mock news for 2025-05-14: Netflix Announces New Comedy Special\n",
            "Cached data to news_stock_cache.csv\n",
            "+----------+-----------------------------------------+--------------------+--------+--------+---------+-----------+-------------+\n",
            "|date      |headline                                 |publishedAt         |genre   |audience|sentiment|price_close|impact       |\n",
            "+----------+-----------------------------------------+--------------------+--------+--------+---------+-----------+-------------+\n",
            "|2025-05-01|No news found                            |NULL                |Other   |General |Neutral  |NULL       |No news found|\n",
            "|2025-05-02|Netflix Announces New Comedy Special     |2025-05-02T13:00:00Z|Comedy  |General |Positive |595.38     |Positive     |\n",
            "|2025-05-03|Non-trading day                          |NULL                |Other   |General |Neutral  |NULL       |Weekend date |\n",
            "|2025-05-04|Non-trading day                          |NULL                |Other   |General |Neutral  |NULL       |Weekend date |\n",
            "|2025-05-05|Netflix Releases New Thriller Series     |2025-05-05T16:00:00Z|Thriller|General |Neutral  |602.18     |Neutral      |\n",
            "|2025-05-06|Netflix Partners for New Series          |2025-05-06T18:00:00Z|Other   |General |Neutral  |597.91     |Neutral      |\n",
            "|2025-05-07|Netflix Stock Surges on New Drama Release|2025-05-07T09:00:00Z|Drama   |General |Neutral  |600.47     |Neutral      |\n",
            "|2025-05-08|No news found                            |NULL                |Other   |General |Neutral  |NULL       |No news found|\n",
            "|2025-05-09|Netflix Expands Youth Programming        |2025-05-09T13:00:00Z|Other   |Youth   |Neutral  |609.55     |Neutral      |\n",
            "|2025-05-10|Non-trading day                          |NULL                |Other   |General |Neutral  |NULL       |Weekend date |\n",
            "|2025-05-11|Non-trading day                          |NULL                |Other   |General |Neutral  |NULL       |Weekend date |\n",
            "|2025-05-12|Netflix Stock Surges on New Drama Release|2025-05-12T10:00:00Z|Drama   |General |Neutral  |607.86     |Neutral      |\n",
            "|2025-05-13|Netflix Partners for New Series          |2025-05-13T09:00:00Z|Other   |General |Neutral  |607.34     |Neutral      |\n",
            "|2025-05-14|Netflix Announces New Comedy Special     |2025-05-14T15:00:00Z|Comedy  |General |Positive |604.28     |Positive     |\n",
            "+----------+-----------------------------------------+--------------------+--------+--------+---------+-----------+-------------+\n",
            "\n",
            "No cache file found: historical_cache.csv\n",
            "Mock news for 2024-04-01: Netflix Releases New Thriller Series\n",
            "Mock news for 2024-04-02: Netflix Partners for New Series\n",
            "No mock news for 2024-04-03\n",
            "Mock news for 2024-04-04: Netflix Announces New Comedy Special\n",
            "Mock news for 2024-04-05: Netflix Partners for New Series\n",
            "Mock news for 2024-04-08: Netflix Expands Youth Programming\n",
            "Mock news for 2024-04-09: Netflix Partners for New Series\n",
            "Mock news for 2024-04-10: Netflix Announces New Comedy Special\n",
            "Mock news for 2024-04-11: Netflix Stock Surges on New Drama Release\n",
            "Mock news for 2024-04-12: Netflix Faces Criticism Over Content\n",
            "Mock news for 2024-04-15: Netflix Releases New Thriller Series\n",
            "Mock news for 2024-04-16: Netflix Releases New Thriller Series\n",
            "No mock news for 2024-04-17\n",
            "Mock news for 2024-04-18: Netflix Partners for New Series\n",
            "No mock news for 2024-04-19\n",
            "No mock news for 2024-04-22\n",
            "No mock news for 2024-04-23\n",
            "Mock news for 2024-04-24: Netflix Partners for New Series\n",
            "Mock news for 2024-04-25: Netflix Announces New Comedy Special\n",
            "Mock news for 2024-04-26: Netflix Faces Criticism Over Content\n",
            "Mock news for 2024-04-29: Netflix Partners for New Series\n",
            "Mock news for 2024-04-30: Netflix Stock Surges on New Drama Release\n",
            "Cached data to historical_cache.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `price_open` cannot be resolved. Did you mean one of the following? [`price_close`, `genre`, `audience`, `date`, `headline`].;\n'Project [date#0, headline#1, sentiment#15, price_close#4, 'price_open]\n+- Filter (date#0 = 2025-05-13)\n   +- Project [date#0, headline#1, publishedAt#2, genre#25, audience#36, sentiment#15, price_close#4, impact#48]\n      +- Project [date#0, headline#1, publishedAt#2, price_open#3, price_close#4, stock_date#5, CASE WHEN isnull(impact#6) THEN calculate_impact(sentiment#15, price_open#3, price_close#4)#47 ELSE impact#6 END AS impact#48, sentiment#15, genre#25, audience#36]\n         +- Project [date#0, headline#1, publishedAt#2, price_open#3, price_close#4, stock_date#5, impact#6, sentiment#15, genre#25, infer_audience(headline#1)#35 AS audience#36]\n            +- Project [date#0, headline#1, publishedAt#2, price_open#3, price_close#4, stock_date#5, impact#6, sentiment#15, infer_genre(headline#1)#24 AS genre#25]\n               +- Project [date#0, headline#1, publishedAt#2, price_open#3, price_close#4, stock_date#5, impact#6, get_sentiment(headline#1)#14 AS sentiment#15]\n                  +- LogicalRDD [date#0, headline#1, publishedAt#2, price_open#3, price_close#4, stock_date#5, impact#6], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-934c71a0fc4a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;31m# Fetch previous day's data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0mprevious_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalysis_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mprevious_date_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"headline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"price_close\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"price_open\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         .withColumn(\"sentiment_score\", \n\u001b[1;32m    347\u001b[0m                     \u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Positive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3225\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m         \"\"\"\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `price_open` cannot be resolved. Did you mean one of the following? [`price_close`, `genre`, `audience`, `date`, `headline`].;\n'Project [date#0, headline#1, sentiment#15, price_close#4, 'price_open]\n+- Filter (date#0 = 2025-05-13)\n   +- Project [date#0, headline#1, publishedAt#2, genre#25, audience#36, sentiment#15, price_close#4, impact#48]\n      +- Project [date#0, headline#1, publishedAt#2, price_open#3, price_close#4, stock_date#5, CASE WHEN isnull(impact#6) THEN calculate_impact(sentiment#15, price_open#3, price_close#4)#47 ELSE impact#6 END AS impact#48, sentiment#15, genre#25, audience#36]\n         +- Project [date#0, headline#1, publishedAt#2, price_open#3, price_close#4, stock_date#5, impact#6, sentiment#15, genre#25, infer_audience(headline#1)#35 AS audience#36]\n            +- Project [date#0, headline#1, publishedAt#2, price_open#3, price_close#4, stock_date#5, impact#6, sentiment#15, infer_genre(headline#1)#24 AS genre#25]\n               +- Project [date#0, headline#1, publishedAt#2, price_open#3, price_close#4, stock_date#5, impact#6, get_sentiment(headline#1)#14 AS sentiment#15]\n                  +- LogicalRDD [date#0, headline#1, publishedAt#2, price_open#3, price_close#4, stock_date#5, impact#6], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pNj0BKgQ3dEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kXPLlB1J4x9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working Code"
      ],
      "metadata": {
        "id": "BmREbkJ7D5TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf, when, lit\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "from datetime import datetime, date, timedelta\n",
        "import random\n",
        "import os\n",
        "\n",
        "# --- Initialize Spark ---\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NewsStockAnalysisPrediction\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "def get_sentiment(text):\n",
        "    if not text or text == 'No headline':\n",
        "        return 'Unknown'\n",
        "    polarity = TextBlob(text).sentiment.polarity\n",
        "    if polarity > 0.2:\n",
        "        return 'Positive'\n",
        "    elif polarity < -0.2:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "def infer_genre(text):\n",
        "    if not text or text == 'No headline':\n",
        "        return 'Unknown'\n",
        "    if 'thriller' in text.lower(): return 'Thriller'\n",
        "    elif 'drama' in text.lower(): return 'Drama'\n",
        "    elif 'comedy' in text.lower(): return 'Comedy'\n",
        "    elif 'romance' in text.lower(): return 'Romance'\n",
        "    else: return 'Other'\n",
        "\n",
        "def infer_audience(text):\n",
        "    if not text or text == 'No headline':\n",
        "        return 'Unknown'\n",
        "    if any(x in text.lower() for x in ['teen', 'gen z', 'youth']): return 'Youth'\n",
        "    elif 'adult' in text.lower(): return 'Adults'\n",
        "    else: return 'General'\n",
        "\n",
        "def calculate_impact(sentiment, open_price, close_price):\n",
        "    if sentiment == 'Unknown' or open_price is None or close_price is None:\n",
        "        return 'Unknown'\n",
        "    change = (close_price - open_price) / open_price * 100\n",
        "    if sentiment == 'Positive' and change > 0:\n",
        "        return 'Positive'\n",
        "    elif sentiment == 'Negative' and change < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "sentiment_udf = udf(get_sentiment, StringType())\n",
        "genre_udf = udf(infer_genre, StringType())\n",
        "audience_udf = udf(infer_audience, StringType())\n",
        "impact_udf = udf(calculate_impact, StringType())\n",
        "\n",
        "\n",
        "def is_weekend(date_obj):\n",
        "    return date_obj.weekday() >= 5\n",
        "\n",
        "def mock_news_by_date(date_str):\n",
        "    \"\"\"Simulate news headlines.\"\"\"\n",
        "    headlines = [\n",
        "        \"Netflix Releases New Thriller Series\",\n",
        "        \"Netflix Announces New Comedy Special\",\n",
        "        \"Netflix Stock Surges on New Drama Release\",\n",
        "        \"Netflix Faces Criticism Over Content\",\n",
        "        \"Netflix Partners for New Series\",\n",
        "        \"Netflix Expands Youth Programming\"\n",
        "    ]\n",
        "    published_at = f\"{date_str}T{random.randint(8, 18):02d}:00:00Z\"\n",
        "    if random.random() < 0.3:\n",
        "        # print(f\"No news for {date_str}\")\n",
        "        return None, None\n",
        "    headline = random.choice(headlines)\n",
        "    # print(f\"News for {date_str}: {headline}\")\n",
        "    return headline, published_at\n",
        "\n",
        "def mock_stock_price(date_str, sentiment=None):\n",
        "    \"\"\"Simulate stock prices, tied to sentiment.\"\"\"\n",
        "    open_price = 600.00 + random.uniform(-10, 10)\n",
        "    if sentiment == \"Positive\":\n",
        "        close_price = open_price + random.uniform(0, 5)\n",
        "    elif sentiment == \"Negative\":\n",
        "        close_price = open_price + random.uniform(-5, 0)\n",
        "    else:\n",
        "        close_price = open_price + random.uniform(-5, 5)\n",
        "    return round(open_price, 2), round(close_price, 2), datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
        "\n",
        "def cache_data(data, filename):\n",
        "    if not data:\n",
        "        # print(f\"No data to cache in {filename}\")\n",
        "        return\n",
        "    try:\n",
        "        pd.DataFrame(data).to_csv(filename, index=False)\n",
        "        # print(f\"Cached data to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error caching data to {filename}: {e}\")\n",
        "\n",
        "def load_cached_data(filename):\n",
        "    if not os.path.exists(filename):\n",
        "        # print(f\"No cache file found: {filename}\")\n",
        "        return None\n",
        "    try:\n",
        "        df = pd.read_csv(filename)\n",
        "        if df.empty or df.columns.empty:\n",
        "            # print(f\"Cache file {filename} is empty or invalid\")\n",
        "            return None\n",
        "        data = df.to_dict('records')\n",
        "        for row in data:\n",
        "            if pd.notnull(row.get('stock_date')):\n",
        "                row['stock_date'] = pd.to_datetime(row['stock_date']).date()\n",
        "        # print(f\"Loaded cached data from {filename}\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading cache file {filename}: {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_news_stock_range(start_date, end_date):\n",
        "    cache_file = \"news_stock_cache.csv\"\n",
        "    cached_data = load_cached_data(cache_file)\n",
        "    if cached_data:\n",
        "        data = cached_data\n",
        "    else:\n",
        "        start_date_obj = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
        "        end_date_obj = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
        "        if start_date_obj > end_date_obj:\n",
        "            print(\"Error: Start date must be before end date\")\n",
        "            return None\n",
        "\n",
        "        data = []\n",
        "        current_date = start_date_obj\n",
        "        while current_date <= end_date_obj:\n",
        "            date_str = current_date.strftime(\"%Y-%m-%d\")\n",
        "            # print(f\"\\nProcessing date: {date_str}\")\n",
        "\n",
        "            if is_weekend(current_date):\n",
        "                data.append((date_str, \"Non-trading day\", None, None, None, None, \"Weekend date\"))\n",
        "                current_date += timedelta(days=1)\n",
        "                continue\n",
        "\n",
        "            headline, published_at = mock_news_by_date(date_str)\n",
        "            sentiment = get_sentiment(headline) if headline else \"Unknown\"\n",
        "            if not headline:\n",
        "                data.append((date_str, \"No news found\", None, None, None, None, \"No news found\"))\n",
        "                current_date += timedelta(days=1)\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                news_dt = datetime.fromisoformat(published_at.replace(\"Z\", \"+00:00\"))\n",
        "                news_date = news_dt.date()\n",
        "                if news_date != current_date:\n",
        "                    # print(f\"News date {news_date} does not match {current_date}\")\n",
        "                    data.append((date_str, headline, published_at, None, None, None, \"News date mismatch\"))\n",
        "                    current_date += timedelta(days=1)\n",
        "                    continue\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing publishedAt: {e}\")\n",
        "                data.append((date_str, headline, published_at, None, None, None, \"Date parsing error\"))\n",
        "                current_date += timedelta(days=1)\n",
        "                continue\n",
        "\n",
        "            open_price, close_price, stock_date = mock_stock_price(date_str, sentiment)\n",
        "            if stock_date is None or stock_date != current_date:\n",
        "                # print(f\"Stock date {stock_date} does not match {current_date}\")\n",
        "                data.append((date_str, headline, published_at, None, None, None, \"Stock date mismatch\"))\n",
        "                current_date += timedelta(days=1)\n",
        "                continue\n",
        "\n",
        "            data.append((date_str, headline, published_at, open_price, close_price, stock_date, None))\n",
        "            current_date += timedelta(days=1)\n",
        "\n",
        "        cache_data(data, cache_file)\n",
        "\n",
        "    schema = StructType([\n",
        "        StructField(\"date\", StringType(), True),\n",
        "        StructField(\"headline\", StringType(), True),\n",
        "        StructField(\"publishedAt\", StringType(), True),\n",
        "        StructField(\"price_open\", DoubleType(), True),\n",
        "        StructField(\"price_close\", DoubleType(), True),\n",
        "        StructField(\"stock_date\", DateType(), True),\n",
        "        StructField(\"impact\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        df = spark.createDataFrame(data, schema)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating DataFrame: {e}\")\n",
        "        return None\n",
        "\n",
        "    df = df.withColumn(\"sentiment\", sentiment_udf(col(\"headline\"))) \\\n",
        "           .withColumn(\"genre\", genre_udf(col(\"headline\"))) \\\n",
        "           .withColumn(\"audience\", audience_udf(col(\"headline\"))) \\\n",
        "           .withColumn(\"impact\",\n",
        "                       when(col(\"impact\").isNull(),\n",
        "                            impact_udf(col(\"sentiment\"), col(\"price_open\"), col(\"price_close\"))\n",
        "                           ).otherwise(col(\"impact\")))\n",
        "\n",
        "    df = df.select(\"date\", \"headline\", \"publishedAt\", \"genre\", \"audience\", \"sentiment\", \"price_open\", \"price_close\", \"impact\")\n",
        "    return df\n",
        "\n",
        "def fetch_historical_data(ticker='NFLX', start_date=\"2024-04-01\", end_date=\"2024-04-30\"):\n",
        "    cache_file = \"historical_cache.csv\"\n",
        "    cached_data = load_cached_data(cache_file)\n",
        "    if cached_data:\n",
        "        stock_data = cached_data\n",
        "    else:\n",
        "        start_date_obj = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
        "        end_date_obj = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
        "        stock_data = []\n",
        "        current_date = start_date_obj\n",
        "        while current_date <= end_date_obj:\n",
        "            date_str = current_date.strftime(\"%Y-%m-%d\")\n",
        "            if is_weekend(current_date):\n",
        "                stock_data.append((date_str, None, None, None, \"Non-trading day\", None))\n",
        "                current_date += timedelta(days=1)\n",
        "                continue\n",
        "            # Fetch mock news\n",
        "            headline, published_at = mock_news_by_date(date_str)\n",
        "            sentiment = get_sentiment(headline) if headline else \"Unknown\"\n",
        "            # Fetch mock stock\n",
        "            open_price, close_price, stock_date = mock_stock_price(date_str, sentiment)\n",
        "            stock_data.append((date_str, open_price, close_price, stock_date, headline or \"No headline\", published_at))\n",
        "            current_date += timedelta(days=1)\n",
        "\n",
        "        cache_data(stock_data, cache_file)\n",
        "\n",
        "    schema = StructType([\n",
        "        StructField(\"date\", StringType(), True),\n",
        "        StructField(\"price_open\", DoubleType(), True),\n",
        "        StructField(\"price_close\", DoubleType(), True),\n",
        "        StructField(\"stock_date\", DateType(), True),\n",
        "        StructField(\"headline\", StringType(), True),\n",
        "        StructField(\"publishedAt\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        stock_df = spark.createDataFrame(stock_data, schema)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating historical DataFrame: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Apply sentiment analysis\n",
        "    stock_df = stock_df.withColumn(\"sentiment\", sentiment_udf(col(\"headline\")))\n",
        "\n",
        "    return stock_df\n",
        "\n",
        "\n",
        "def train_stock_prediction_model(historical_data):\n",
        "    if historical_data is None:\n",
        "        print(\"No historical data to train model\")\n",
        "        return None, None\n",
        "\n",
        "    # Prepare features: sentiment score, previous day's close, open\n",
        "    data = historical_data.withColumn(\"sentiment_score\",\n",
        "        when(col(\"sentiment\") == \"Positive\", 1.0)\n",
        "        .when(col(\"sentiment\") == \"Negative\", -1.0)\n",
        "        .otherwise(0.0))\n",
        "\n",
        "    # Create feature vector\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=[\"sentiment_score\", \"price_close\", \"price_open\"],\n",
        "        outputCol=\"features\",\n",
        "        handleInvalid=\"skip\"\n",
        "    )\n",
        "\n",
        "    # Shift data to predict next day's close\n",
        "    from pyspark.sql.window import Window\n",
        "    from pyspark.sql.functions import lag\n",
        "    w = Window.orderBy(\"date\")\n",
        "    data = data.withColumn(\"next_close\", lag(\"price_close\", -1).over(w))\n",
        "\n",
        "    data = data.dropna(subset=[\"next_close\", \"price_close\", \"price_open\", \"sentiment_score\"])\n",
        "\n",
        "    if data.count() == 0:\n",
        "        print(\"No valid data for training after cleaning\")\n",
        "        return None, None\n",
        "\n",
        "    data = assembler.transform(data)\n",
        "\n",
        "    # Train Linear Regression\n",
        "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"next_close\")\n",
        "    model = lr.fit(data)\n",
        "    return model, assembler\n",
        "\n",
        "\n",
        "start_date = \"2025-05-01\"\n",
        "end_date = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
        "# end_date = \"2025-05-14\"  # Includes current date (2025-05-14)\n",
        "analysis_df = analyze_news_stock_range(start_date, end_date)\n",
        "if analysis_df is not None:\n",
        "    analysis_df.show(truncate=False)\n",
        "else:\n",
        "    print(\"Failed to create analysis DataFrame\")\n",
        "\n",
        "# Train prediction model\n",
        "historical_df = fetch_historical_data('NFLX', \"2024-04-01\", \"2024-04-30\")\n",
        "model, assembler = train_stock_prediction_model(historical_df)\n",
        "\n",
        "# Dynamic prediction based on current date\n",
        "if model is not None and assembler is not None:\n",
        "    current_date = datetime.now().date()\n",
        "    previous_date = current_date - timedelta(days=1)\n",
        "    previous_date_str = previous_date.strftime(\"%Y-%m-%d\")\n",
        "    tomorrow_date = current_date + timedelta(days=1)\n",
        "\n",
        "    # Fetch previous day's data\n",
        "    previous_data = analysis_df.filter(col(\"date\") == previous_date_str) \\\n",
        "        .select(\"date\", \"headline\", \"sentiment\", \"price_close\", \"price_open\") \\\n",
        "        .withColumn(\"sentiment_score\",\n",
        "                    when(col(\"sentiment\") == \"Positive\", 1.0)\n",
        "                    .when(col(\"sentiment\") == \"Negative\", -1.0)\n",
        "                    .otherwise(0.0)) \\\n",
        "        .select(\"date\", \"headline\", \"sentiment\", \"price_close\", \"price_open\", \"sentiment_score\")\n",
        "\n",
        "    if previous_data.count() > 0:\n",
        "        # Convert to pandas for easier access\n",
        "        previous_row = previous_data.toPandas().iloc[0]\n",
        "\n",
        "        # Print previous day's data\n",
        "        print(f\"\\n📅 Data for {previous_date}:\")\n",
        "        print(f\"📰 Headline: {previous_row['headline']}\")\n",
        "        print(f\"🧠 Sentiment: {previous_row['sentiment']}\")\n",
        "        print(f\"📈 Close Price: ${previous_row['price_close']:.2f}\")\n",
        "\n",
        "        # Prepare features for prediction\n",
        "        latest_features = assembler.transform(previous_data)\n",
        "        prediction = model.transform(latest_features)\n",
        "        predicted_price = prediction.select(\"prediction\").collect()[0][0]\n",
        "\n",
        "        # Print prediction for tomorrow\n",
        "        print(f\"\\n📅 Prediction for {tomorrow_date}:\")\n",
        "        print(f\"📰 Based on Headline from {previous_date}: {previous_row['headline']}\")\n",
        "        print(f\"🧠 Sentiment: {previous_row['sentiment']}\")\n",
        "        print(f\"📈 Predicted Close Price: ${predicted_price:.2f}\")\n",
        "    else:\n",
        "        print(f\"No valid data for {previous_date_str} to make a prediction\")\n",
        "else:\n",
        "    print(\"Failed to train prediction model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJE29o-04yA1",
        "outputId": "3a6457a4-a20e-46bf-d860-5150b0c6b500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------------------------------+--------------------+--------+--------+---------+----------+-----------+-------------+\n",
            "|date      |headline                                 |publishedAt         |genre   |audience|sentiment|price_open|price_close|impact       |\n",
            "+----------+-----------------------------------------+--------------------+--------+--------+---------+----------+-----------+-------------+\n",
            "|2025-05-01|No news found                            |NULL                |Other   |General |Neutral  |NULL      |NULL       |No news found|\n",
            "|2025-05-02|Netflix Releases New Thriller Series     |2025-05-02T12:00:00Z|Thriller|General |Neutral  |591.91    |590.06     |Neutral      |\n",
            "|2025-05-03|Non-trading day                          |NULL                |Other   |General |Neutral  |NULL      |NULL       |Weekend date |\n",
            "|2025-05-04|Non-trading day                          |NULL                |Other   |General |Neutral  |NULL      |NULL       |Weekend date |\n",
            "|2025-05-05|Netflix Stock Surges on New Drama Release|2025-05-05T18:00:00Z|Drama   |General |Neutral  |599.23    |595.04     |Neutral      |\n",
            "|2025-05-06|Netflix Faces Criticism Over Content     |2025-05-06T18:00:00Z|Other   |General |Neutral  |606.16    |603.95     |Neutral      |\n",
            "|2025-05-07|Netflix Announces New Comedy Special     |2025-05-07T11:00:00Z|Comedy  |General |Positive |606.65    |611.12     |Positive     |\n",
            "|2025-05-08|Netflix Partners for New Series          |2025-05-08T13:00:00Z|Other   |General |Neutral  |604.81    |600.82     |Neutral      |\n",
            "|2025-05-09|Netflix Partners for New Series          |2025-05-09T13:00:00Z|Other   |General |Neutral  |609.64    |613.25     |Neutral      |\n",
            "|2025-05-10|Non-trading day                          |NULL                |Other   |General |Neutral  |NULL      |NULL       |Weekend date |\n",
            "|2025-05-11|Non-trading day                          |NULL                |Other   |General |Neutral  |NULL      |NULL       |Weekend date |\n",
            "|2025-05-12|Netflix Releases New Thriller Series     |2025-05-12T18:00:00Z|Thriller|General |Neutral  |593.48    |596.67     |Neutral      |\n",
            "|2025-05-13|Netflix Expands Youth Programming        |2025-05-13T08:00:00Z|Other   |Youth   |Neutral  |602.22    |601.82     |Neutral      |\n",
            "+----------+-----------------------------------------+--------------------+--------+--------+---------+----------+-----------+-------------+\n",
            "\n",
            "\n",
            "📅 Data for 2025-05-13:\n",
            "📰 Headline: Netflix Expands Youth Programming\n",
            "🧠 Sentiment: Neutral\n",
            "📈 Close Price: $601.82\n",
            "\n",
            "📅 Prediction for 2025-05-15:\n",
            "📰 Based on Headline from 2025-05-13: Netflix Expands Youth Programming\n",
            "🧠 Sentiment: Neutral\n",
            "📈 Predicted Close Price: $599.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sEElMqwm427A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jlxoNipzxkDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## py file code"
      ],
      "metadata": {
        "id": "It94zPUXxlR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf, when, lit\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "from datetime import datetime, date, timedelta\n",
        "import random\n",
        "import os\n",
        "\n",
        "# --- Initialize Spark ---\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NewsStockAnalysisPrediction\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "def get_sentiment(text):\n",
        "    if not text or text == 'No headline':\n",
        "        return 'Unknown'\n",
        "    polarity = TextBlob(text).sentiment.polarity\n",
        "    if polarity > 0.2:\n",
        "        return 'Positive'\n",
        "    elif polarity < -0.2:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "def infer_genre(text):\n",
        "    if not text or text == 'No headline':\n",
        "        return 'Unknown'\n",
        "    if 'thriller' in text.lower(): return 'Thriller'\n",
        "    elif 'drama' in text.lower(): return 'Drama'\n",
        "    elif 'comedy' in text.lower(): return 'Comedy'\n",
        "    elif 'romance' in text.lower(): return 'Romance'\n",
        "    else: return 'Other'\n",
        "\n",
        "def infer_audience(text):\n",
        "    if not text or text == 'No headline':\n",
        "        return 'Unknown'\n",
        "    if any(x in text.lower() for x in ['teen', 'gen z', 'youth']): return 'Youth'\n",
        "    elif 'adult' in text.lower(): return 'Adults'\n",
        "    else: return 'General'\n",
        "\n",
        "def calculate_impact(sentiment, open_price, close_price):\n",
        "    if sentiment == 'Unknown' or open_price is None or close_price is None:\n",
        "        return 'Unknown'\n",
        "    change = (close_price - open_price) / open_price * 100\n",
        "    if sentiment == 'Positive' and change > 0:\n",
        "        return 'Positive'\n",
        "    elif sentiment == 'Negative' and change < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "sentiment_udf = udf(get_sentiment, StringType())\n",
        "genre_udf = udf(infer_genre, StringType())\n",
        "audience_udf = udf(infer_audience, StringType())\n",
        "impact_udf = udf(calculate_impact, StringType())\n",
        "\n",
        "\n",
        "def is_weekend(date_obj):\n",
        "    return date_obj.weekday() >= 5\n",
        "\n",
        "def mock_news_by_date(date_str):\n",
        "    \"\"\"Simulate news headlines.\"\"\"\n",
        "    headlines = [\n",
        "        \"Netflix Releases New Thriller Series\",\n",
        "        \"Netflix Announces New Comedy Special\",\n",
        "        \"Netflix Stock Surges on New Drama Release\",\n",
        "        \"Netflix Faces Criticism Over Content\",\n",
        "        \"Netflix Partners for New Series\",\n",
        "        \"Netflix Expands Youth Programming\"\n",
        "    ]\n",
        "    published_at = f\"{date_str}T{random.randint(8, 18):02d}:00:00Z\"\n",
        "    if random.random() < 0.3:\n",
        "        # print(f\"No news for {date_str}\")\n",
        "        return None, None\n",
        "    headline = random.choice(headlines)\n",
        "    # print(f\"News for {date_str}: {headline}\")\n",
        "    return headline, published_at\n",
        "\n",
        "def mock_stock_price(date_str, sentiment=None):\n",
        "    \"\"\"Simulate stock prices, tied to sentiment.\"\"\"\n",
        "    open_price = 600.00 + random.uniform(-10, 10)\n",
        "    if sentiment == \"Positive\":\n",
        "        close_price = open_price + random.uniform(0, 5)\n",
        "    elif sentiment == \"Negative\":\n",
        "        close_price = open_price + random.uniform(-5, 0)\n",
        "    else:\n",
        "        close_price = open_price + random.uniform(-5, 5)\n",
        "    return round(open_price, 2), round(close_price, 2), datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
        "\n",
        "def cache_data(data, filename):\n",
        "    if not data:\n",
        "        # print(f\"No data to cache in {filename}\")\n",
        "        return\n",
        "    try:\n",
        "        pd.DataFrame(data).to_csv(filename, index=False)\n",
        "        # print(f\"Cached data to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error caching data to {filename}: {e}\")\n",
        "\n",
        "def load_cached_data(filename):\n",
        "    if not os.path.exists(filename):\n",
        "        # print(f\"No cache file found: {filename}\")\n",
        "        return None\n",
        "    try:\n",
        "        df = pd.read_csv(filename)\n",
        "        if df.empty or df.columns.empty:\n",
        "            # print(f\"Cache file {filename} is empty or invalid\")\n",
        "            return None\n",
        "        data = df.to_dict('records')\n",
        "        for row in data:\n",
        "            if pd.notnull(row.get('stock_date')):\n",
        "                row['stock_date'] = pd.to_datetime(row['stock_date']).date()\n",
        "        # print(f\"Loaded cached data from {filename}\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading cache file {filename}: {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_news_stock_range(start_date, end_date):\n",
        "    cache_file = \"news_stock_cache.csv\"\n",
        "    cached_data = load_cached_data(cache_file)\n",
        "    if cached_data:\n",
        "        data = cached_data\n",
        "    else:\n",
        "        start_date_obj = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
        "        end_date_obj = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
        "        if start_date_obj > end_date_obj:\n",
        "            print(\"Error: Start date must be before end date\")\n",
        "            return None\n",
        "\n",
        "        data = []\n",
        "        current_date = start_date_obj\n",
        "        while current_date <= end_date_obj:\n",
        "            date_str = current_date.strftime(\"%Y-%m-%d\")\n",
        "            # print(f\"\\nProcessing date: {date_str}\")\n",
        "\n",
        "            if is_weekend(current_date):\n",
        "                data.append((date_str, \"Non-trading day\", None, None, None, None, \"Weekend date\"))\n",
        "                current_date += timedelta(days=1)\n",
        "                continue\n",
        "\n",
        "            headline, published_at = mock_news_by_date(date_str)\n",
        "            sentiment = get_sentiment(headline) if headline else \"Unknown\"\n",
        "            if not headline:\n",
        "                data.append((date_str, \"No news found\", None, None, None, None, \"No news found\"))\n",
        "                current_date += timedelta(days=1)\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                news_dt = datetime.fromisoformat(published_at.replace(\"Z\", \"+00:00\"))\n",
        "                news_date = news_dt.date()\n",
        "                if news_date != current_date:\n",
        "                    # print(f\"News date {news_date} does not match {current_date}\")\n",
        "                    data.append((date_str, headline, published_at, None, None, None, \"News date mismatch\"))\n",
        "                    current_date += timedelta(days=1)\n",
        "                    continue\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing publishedAt: {e}\")\n",
        "                data.append((date_str, headline, published_at, None, None, None, \"Date parsing error\"))\n",
        "                current_date += timedelta(days=1)\n",
        "                continue\n",
        "\n",
        "            open_price, close_price, stock_date = mock_stock_price(date_str, sentiment)\n",
        "            if stock_date is None or stock_date != current_date:\n",
        "                # print(f\"Stock date {stock_date} does not match {current_date}\")\n",
        "                data.append((date_str, headline, published_at, None, None, None, \"Stock date mismatch\"))\n",
        "                current_date += timedelta(days=1)\n",
        "                continue\n",
        "\n",
        "            data.append((date_str, headline, published_at, open_price, close_price, stock_date, None))\n",
        "            current_date += timedelta(days=1)\n",
        "\n",
        "        cache_data(data, cache_file)\n",
        "\n",
        "    schema = StructType([\n",
        "        StructField(\"date\", StringType(), True),\n",
        "        StructField(\"headline\", StringType(), True),\n",
        "        StructField(\"publishedAt\", StringType(), True),\n",
        "        StructField(\"price_open\", DoubleType(), True),\n",
        "        StructField(\"price_close\", DoubleType(), True),\n",
        "        StructField(\"stock_date\", DateType(), True),\n",
        "        StructField(\"impact\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        df = spark.createDataFrame(data, schema)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating DataFrame: {e}\")\n",
        "        return None\n",
        "\n",
        "    df = df.withColumn(\"sentiment\", sentiment_udf(col(\"headline\"))) \\\n",
        "           .withColumn(\"genre\", genre_udf(col(\"headline\"))) \\\n",
        "           .withColumn(\"audience\", audience_udf(col(\"headline\"))) \\\n",
        "           .withColumn(\"impact\",\n",
        "                       when(col(\"impact\").isNull(),\n",
        "                            impact_udf(col(\"sentiment\"), col(\"price_open\"), col(\"price_close\"))\n",
        "                           ).otherwise(col(\"impact\")))\n",
        "\n",
        "    df = df.select(\"date\", \"headline\", \"publishedAt\", \"genre\", \"audience\", \"sentiment\", \"price_open\", \"price_close\", \"impact\")\n",
        "    return df\n",
        "\n",
        "def fetch_historical_data(ticker='NFLX', start_date=\"2024-04-01\", end_date=\"2024-04-30\"):\n",
        "    cache_file = \"historical_cache.csv\"\n",
        "    cached_data = load_cached_data(cache_file)\n",
        "    if cached_data:\n",
        "        stock_data = cached_data\n",
        "    else:\n",
        "        start_date_obj = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
        "        end_date_obj = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
        "        stock_data = []\n",
        "        current_date = start_date_obj\n",
        "        while current_date <= end_date_obj:\n",
        "            date_str = current_date.strftime(\"%Y-%m-%d\")\n",
        "            if is_weekend(current_date):\n",
        "                stock_data.append((date_str, None, None, None, \"Non-trading day\", None))\n",
        "                current_date += timedelta(days=1)\n",
        "                continue\n",
        "            # Fetch mock news\n",
        "            headline, published_at = mock_news_by_date(date_str)\n",
        "            sentiment = get_sentiment(headline) if headline else \"Unknown\"\n",
        "            # Fetch mock stock\n",
        "            open_price, close_price, stock_date = mock_stock_price(date_str, sentiment)\n",
        "            stock_data.append((date_str, open_price, close_price, stock_date, headline or \"No headline\", published_at))\n",
        "            current_date += timedelta(days=1)\n",
        "\n",
        "        cache_data(stock_data, cache_file)\n",
        "\n",
        "    schema = StructType([\n",
        "        StructField(\"date\", StringType(), True),\n",
        "        StructField(\"price_open\", DoubleType(), True),\n",
        "        StructField(\"price_close\", DoubleType(), True),\n",
        "        StructField(\"stock_date\", DateType(), True),\n",
        "        StructField(\"headline\", StringType(), True),\n",
        "        StructField(\"publishedAt\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        stock_df = spark.createDataFrame(stock_data, schema)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating historical DataFrame: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Apply sentiment analysis\n",
        "    stock_df = stock_df.withColumn(\"sentiment\", sentiment_udf(col(\"headline\")))\n",
        "\n",
        "    return stock_df\n",
        "\n",
        "\n",
        "def train_stock_prediction_model(historical_data):\n",
        "    if historical_data is None:\n",
        "        print(\"No historical data to train model\")\n",
        "        return None, None\n",
        "\n",
        "    # Prepare features: sentiment score, previous day's close, open\n",
        "    data = historical_data.withColumn(\"sentiment_score\",\n",
        "        when(col(\"sentiment\") == \"Positive\", 1.0)\n",
        "        .when(col(\"sentiment\") == \"Negative\", -1.0)\n",
        "        .otherwise(0.0))\n",
        "\n",
        "    # Create feature vector\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=[\"sentiment_score\", \"price_close\", \"price_open\"],\n",
        "        outputCol=\"features\",\n",
        "        handleInvalid=\"skip\"\n",
        "    )\n",
        "\n",
        "    # Shift data to predict next day's close\n",
        "    from pyspark.sql.window import Window\n",
        "    from pyspark.sql.functions import lag\n",
        "    w = Window.orderBy(\"date\")\n",
        "    data = data.withColumn(\"next_close\", lag(\"price_close\", -1).over(w))\n",
        "\n",
        "    data = data.dropna(subset=[\"next_close\", \"price_close\", \"price_open\", \"sentiment_score\"])\n",
        "\n",
        "    if data.count() == 0:\n",
        "        print(\"No valid data for training after cleaning\")\n",
        "        return None, None\n",
        "\n",
        "    data = assembler.transform(data)\n",
        "\n",
        "    # Train Linear Regression\n",
        "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"next_close\")\n",
        "    model = lr.fit(data)\n",
        "    return model, assembler\n",
        "\n",
        "\n",
        "start_date = \"2025-05-01\"\n",
        "end_date = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
        "# end_date = \"2025-05-14\"  # Includes current date (2025-05-14)\n",
        "analysis_df = analyze_news_stock_range(start_date, end_date)\n",
        "if analysis_df is not None:\n",
        "    analysis_df.show(truncate=False)\n",
        "    finall_df = analysis_df.toPandas()\n",
        "else:\n",
        "    print(\"Failed to create analysis DataFrame\")\n",
        "\n",
        "# Train prediction model\n",
        "historical_df = fetch_historical_data('NFLX', \"2024-04-01\", \"2024-04-30\")\n",
        "model, assembler = train_stock_prediction_model(historical_df)\n",
        "\n",
        "# Dynamic prediction based on current date\n",
        "if model is not None and assembler is not None:\n",
        "    current_date = datetime.now().date()\n",
        "    previous_date = current_date - timedelta(days=1)\n",
        "    previous_date_str = previous_date.strftime(\"%Y-%m-%d\")\n",
        "    tomorrow_date = current_date + timedelta(days=1)\n",
        "\n",
        "    # Fetch previous day's data\n",
        "    previous_data = analysis_df.filter(col(\"date\") == previous_date_str) \\\n",
        "        .select(\"date\", \"headline\", \"sentiment\", \"price_close\", \"price_open\") \\\n",
        "        .withColumn(\"sentiment_score\",\n",
        "                    when(col(\"sentiment\") == \"Positive\", 1.0)\n",
        "                    .when(col(\"sentiment\") == \"Negative\", -1.0)\n",
        "                    .otherwise(0.0)) \\\n",
        "        .select(\"date\", \"headline\", \"sentiment\", \"price_close\", \"price_open\", \"sentiment_score\")\n",
        "\n",
        "    if previous_data.count() > 0:\n",
        "        # Convert to pandas for easier access\n",
        "        previous_row = previous_data.toPandas().iloc[0]\n",
        "\n",
        "        # Print previous day's data\n",
        "        print(f\"\\n📅 Data for {previous_date}:\")\n",
        "        print(f\"📰 Headline: {previous_row['headline']}\")\n",
        "        print(f\"🧠 Sentiment: {previous_row['sentiment']}\")\n",
        "        print(f\"📈 Close Price: ${previous_row['price_close']:.2f}\")\n",
        "\n",
        "        # Prepare features for prediction\n",
        "        latest_features = assembler.transform(previous_data)\n",
        "        prediction = model.transform(latest_features)\n",
        "        predicted_price = prediction.select(\"prediction\").collect()[0][0]\n",
        "\n",
        "        # Print prediction for tomorrow\n",
        "        print(f\"\\n📅 Prediction for {tomorrow_date}:\")\n",
        "        print(f\"📰 Based on Headline from {previous_date}: {previous_row['headline']}\")\n",
        "        print(f\"🧠 Sentiment: {previous_row['sentiment']}\")\n",
        "        print(f\"📈 Predicted Close Price: ${predicted_price:.2f}\")\n",
        "    else:\n",
        "        print(f\"No valid data for {previous_date_str} to make a prediction\")\n",
        "else:\n",
        "    print(\"Failed to train prediction model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53gkES2exkKw",
        "outputId": "2a9d7f29-b4bb-497a-92fd-cf3667232db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------------------------------+--------------------+--------+--------+---------+----------+-----------+-------------+\n",
            "|date      |headline                                 |publishedAt         |genre   |audience|sentiment|price_open|price_close|impact       |\n",
            "+----------+-----------------------------------------+--------------------+--------+--------+---------+----------+-----------+-------------+\n",
            "|2025-05-01|Netflix Stock Surges on New Drama Release|2025-05-01T08:00:00Z|Drama   |General |Neutral  |595.98    |597.23     |Neutral      |\n",
            "|2025-05-02|No news found                            |NULL                |Other   |General |Neutral  |NULL      |NULL       |No news found|\n",
            "|2025-05-03|Non-trading day                          |NULL                |Other   |General |Neutral  |NULL      |NULL       |Weekend date |\n",
            "|2025-05-04|Non-trading day                          |NULL                |Other   |General |Neutral  |NULL      |NULL       |Weekend date |\n",
            "|2025-05-05|Netflix Partners for New Series          |2025-05-05T12:00:00Z|Other   |General |Neutral  |596.3     |592.78     |Neutral      |\n",
            "|2025-05-06|Netflix Expands Youth Programming        |2025-05-06T16:00:00Z|Other   |Youth   |Neutral  |596.14    |596.16     |Neutral      |\n",
            "|2025-05-07|No news found                            |NULL                |Other   |General |Neutral  |NULL      |NULL       |No news found|\n",
            "|2025-05-08|No news found                            |NULL                |Other   |General |Neutral  |NULL      |NULL       |No news found|\n",
            "|2025-05-09|Netflix Stock Surges on New Drama Release|2025-05-09T17:00:00Z|Drama   |General |Neutral  |597.85    |598.17     |Neutral      |\n",
            "|2025-05-10|Non-trading day                          |NULL                |Other   |General |Neutral  |NULL      |NULL       |Weekend date |\n",
            "|2025-05-11|Non-trading day                          |NULL                |Other   |General |Neutral  |NULL      |NULL       |Weekend date |\n",
            "|2025-05-12|Netflix Announces New Comedy Special     |2025-05-12T17:00:00Z|Comedy  |General |Positive |595.35    |596.0      |Positive     |\n",
            "|2025-05-13|No news found                            |NULL                |Other   |General |Neutral  |NULL      |NULL       |No news found|\n",
            "|2025-05-14|Netflix Releases New Thriller Series     |2025-05-14T11:00:00Z|Thriller|General |Neutral  |590.14    |591.35     |Neutral      |\n",
            "|2025-05-15|Netflix Partners for New Series          |2025-05-15T17:00:00Z|Other   |General |Neutral  |592.68    |589.27     |Neutral      |\n",
            "|2025-05-16|No news found                            |NULL                |Other   |General |Neutral  |NULL      |NULL       |No news found|\n",
            "|2025-05-17|Non-trading day                          |NULL                |Other   |General |Neutral  |NULL      |NULL       |Weekend date |\n",
            "|2025-05-18|Non-trading day                          |NULL                |Other   |General |Neutral  |NULL      |NULL       |Weekend date |\n",
            "|2025-05-19|Netflix Expands Youth Programming        |2025-05-19T09:00:00Z|Other   |Youth   |Neutral  |593.41    |589.3      |Neutral      |\n",
            "|2025-05-20|Netflix Expands Youth Programming        |2025-05-20T13:00:00Z|Other   |Youth   |Neutral  |592.95    |589.9      |Neutral      |\n",
            "+----------+-----------------------------------------+--------------------+--------+--------+---------+----------+-----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "No valid data for training after cleaning\n",
            "Failed to train prediction model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finall_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "iOW0BiAM1pG0",
        "outputId": "d0fbf18a-b10d-4a2e-a9e0-670e583adf09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         date                                   headline  \\\n",
              "0  2025-05-01  Netflix Stock Surges on New Drama Release   \n",
              "1  2025-05-02                              No news found   \n",
              "2  2025-05-03                            Non-trading day   \n",
              "3  2025-05-04                            Non-trading day   \n",
              "4  2025-05-05            Netflix Partners for New Series   \n",
              "\n",
              "            publishedAt  genre audience sentiment  price_open  price_close  \\\n",
              "0  2025-05-01T08:00:00Z  Drama  General   Neutral      595.98       597.23   \n",
              "1                  None  Other  General   Neutral         NaN          NaN   \n",
              "2                  None  Other  General   Neutral         NaN          NaN   \n",
              "3                  None  Other  General   Neutral         NaN          NaN   \n",
              "4  2025-05-05T12:00:00Z  Other  General   Neutral      596.30       592.78   \n",
              "\n",
              "          impact  \n",
              "0        Neutral  \n",
              "1  No news found  \n",
              "2   Weekend date  \n",
              "3   Weekend date  \n",
              "4        Neutral  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-021bdfa6-b21d-4082-ac44-a8a09fed15fc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>headline</th>\n",
              "      <th>publishedAt</th>\n",
              "      <th>genre</th>\n",
              "      <th>audience</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>price_open</th>\n",
              "      <th>price_close</th>\n",
              "      <th>impact</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-05-01</td>\n",
              "      <td>Netflix Stock Surges on New Drama Release</td>\n",
              "      <td>2025-05-01T08:00:00Z</td>\n",
              "      <td>Drama</td>\n",
              "      <td>General</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>595.98</td>\n",
              "      <td>597.23</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-05-02</td>\n",
              "      <td>No news found</td>\n",
              "      <td>None</td>\n",
              "      <td>Other</td>\n",
              "      <td>General</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No news found</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-05-03</td>\n",
              "      <td>Non-trading day</td>\n",
              "      <td>None</td>\n",
              "      <td>Other</td>\n",
              "      <td>General</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Weekend date</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-05-04</td>\n",
              "      <td>Non-trading day</td>\n",
              "      <td>None</td>\n",
              "      <td>Other</td>\n",
              "      <td>General</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Weekend date</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-05-05</td>\n",
              "      <td>Netflix Partners for New Series</td>\n",
              "      <td>2025-05-05T12:00:00Z</td>\n",
              "      <td>Other</td>\n",
              "      <td>General</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>596.30</td>\n",
              "      <td>592.78</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-021bdfa6-b21d-4082-ac44-a8a09fed15fc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-021bdfa6-b21d-4082-ac44-a8a09fed15fc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-021bdfa6-b21d-4082-ac44-a8a09fed15fc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-dd9568d2-4594-470e-9381-5841b5b21843\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dd9568d2-4594-470e-9381-5841b5b21843')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-dd9568d2-4594-470e-9381-5841b5b21843 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "finall_df",
              "summary": "{\n  \"name\": \"finall_df\",\n  \"rows\": 29,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"2025-05-28\",\n          \"2025-05-17\",\n          \"2025-05-13\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"headline\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Netflix Stock Surges on New Drama Release\",\n          \"No news found\",\n          \"Netflix Announces New Comedy Special\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"publishedAt\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"2025-05-28T13:00:00Z\",\n          \"2025-05-21T14:00:00Z\",\n          \"2025-05-01T08:00:00Z\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"genre\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Other\",\n          \"Thriller\",\n          \"Drama\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audience\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Youth\",\n          \"General\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Positive\",\n          \"Neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"price_open\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.7404822807102978,\n        \"min\": 590.14,\n        \"max\": 604.04,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          601.44,\n          604.04\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"price_close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.324043246777254,\n        \"min\": 589.27,\n        \"max\": 601.8,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          601.8,\n          600.01\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"impact\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"No news found\",\n          \"Positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finall_df.to_csv('analysis_data.csv', index=False)"
      ],
      "metadata": {
        "id": "GDlVJlge3z9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6c9FTAWm4HhQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}